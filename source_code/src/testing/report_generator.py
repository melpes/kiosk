"""
í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±ê¸°
"""

import os
from typing import Dict, List, Any
from datetime import datetime
from pathlib import Path

from ..models.testing_models import TestAnalysis, TestResult, TestCaseCategory
from ..logger import get_logger

class ReportGenerator:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±ê¸°"""
    
    def __init__(self, output_directory: str = "test_results"):
        self.output_directory = Path(output_directory)
        self.logger = get_logger(__name__)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_directory.mkdir(parents=True, exist_ok=True)
        self.reports_dir = self.output_directory / "reports"
        self.reports_dir.mkdir(exist_ok=True)
    
    def generate_text_report(self, analysis: TestAnalysis, output_path: str = None) -> str:
        """
        í…ìŠ¤íŠ¸ í˜•íƒœì˜ ë³´ê³ ì„œ ìƒì„±
        
        Args:
            analysis: ë¶„ì„ ê²°ê³¼
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
            
        Returns:
            str: ìƒì„±ëœ íŒŒì¼ ê²½ë¡œ
        """
        try:
            if output_path is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_path = self.reports_dir / f"test_report_{timestamp}.txt"
            else:
                output_path = Path(output_path)
            
            self.logger.info(f"í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„± ì‹œì‘: {output_path}")
            
            # ë³´ê³ ì„œ ë‚´ìš© ìƒì„±
            content = self._generate_text_content(analysis)
            
            # íŒŒì¼ ì €ì¥
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self.logger.info(f"í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {output_path}")
            return str(output_path)
            
        except Exception as e:
            self.logger.error(f"í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def generate_markdown_report(self, analysis: TestAnalysis, output_path: str = None) -> str:
        """
        ë§ˆí¬ë‹¤ìš´ í˜•íƒœì˜ ë³´ê³ ì„œ ìƒì„±
        
        Args:
            analysis: ë¶„ì„ ê²°ê³¼
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
            
        Returns:
            str: ìƒì„±ëœ íŒŒì¼ ê²½ë¡œ
        """
        try:
            if output_path is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_path = self.reports_dir / f"test_report_{timestamp}.md"
            else:
                output_path = Path(output_path)
            
            self.logger.info(f"ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„± ì‹œì‘: {output_path}")
            
            # ë³´ê³ ì„œ ë‚´ìš© ìƒì„±
            content = self._generate_markdown_content(analysis)
            
            # íŒŒì¼ ì €ì¥
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self.logger.info(f"ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {output_path}")
            return str(output_path)
            
        except Exception as e:
            self.logger.error(f"ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def generate_summary_report(self, analysis: TestAnalysis) -> str:
        """
        ìš”ì•½ ë³´ê³ ì„œ ìƒì„± (ì½˜ì†” ì¶œë ¥ìš©)
        
        Args:
            analysis: ë¶„ì„ ê²°ê³¼
            
        Returns:
            str: ìš”ì•½ ë³´ê³ ì„œ ë‚´ìš©
        """
        lines = []
        lines.append("=" * 60)
        lines.append("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        lines.append("=" * 60)
        lines.append("")
        
        # ê¸°ë³¸ í†µê³„
        lines.append("ğŸ“Š ê¸°ë³¸ í†µê³„")
        lines.append(f"  â€¢ ì „ì²´ í…ŒìŠ¤íŠ¸: {analysis.total_tests}ê°œ")
        lines.append(f"  â€¢ ì„±ê³µë¥ : {analysis.success_rate:.2%}")
        lines.append(f"  â€¢ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {analysis.average_processing_time:.3f}ì´ˆ")
        lines.append("")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥
        if analysis.category_performance:
            lines.append("ğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥")
            for category, performance in sorted(analysis.category_performance.items()):
                lines.append(f"  â€¢ {category}: {performance:.2%}")
            lines.append("")
        
        # ì£¼ìš” ì˜¤ë¥˜
        if analysis.error_summary:
            lines.append("âŒ ì£¼ìš” ì˜¤ë¥˜")
            sorted_errors = sorted(analysis.error_summary.items(), key=lambda x: x[1], reverse=True)
            for error_type, count in sorted_errors[:3]:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                lines.append(f"  â€¢ {error_type}: {count}íšŒ")
            lines.append("")
        
        lines.append("=" * 60)
        
        return "\n".join(lines)
    
    def _generate_text_content(self, analysis: TestAnalysis) -> str:
        """í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ë‚´ìš© ìƒì„±"""
        lines = []
        
        # í—¤ë”
        lines.append("=" * 80)
        lines.append("ìŒì„± í‚¤ì˜¤ìŠ¤í¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë³´ê³ ì„œ")
        lines.append("=" * 80)
        lines.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")
        
        # ì „ì²´ ìš”ì•½
        lines.append("1. ì „ì²´ ìš”ì•½")
        lines.append("-" * 40)
        lines.append(f"ì´ í…ŒìŠ¤íŠ¸ ìˆ˜: {analysis.total_tests}")
        lines.append(f"ì„±ê³µë¥ : {analysis.success_rate:.2%}")
        lines.append(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {analysis.average_processing_time:.3f}ì´ˆ")
        lines.append(f"ì´ ì˜¤ë¥˜ ìˆ˜: {sum(analysis.error_summary.values())}")
        lines.append("")
        
        # ì˜ë„ë³„ ì •í™•ë„
        if analysis.intent_accuracy:
            lines.append("2. ì˜ë„ë³„ ì •í™•ë„")
            lines.append("-" * 40)
            for intent, accuracy in sorted(analysis.intent_accuracy.items()):
                lines.append(f"{intent}: {accuracy:.2%}")
            lines.append("")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥
        if analysis.category_performance:
            lines.append("3. ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥")
            lines.append("-" * 40)
            for category, performance in sorted(analysis.category_performance.items()):
                lines.append(f"{category}: {performance:.2%}")
            lines.append("")
        
        # ì˜¤ë¥˜ ë¶„ì„
        if analysis.error_summary:
            lines.append("4. ì˜¤ë¥˜ ë¶„ì„")
            lines.append("-" * 40)
            sorted_errors = sorted(analysis.error_summary.items(), key=lambda x: x[1], reverse=True)
            for error_type, count in sorted_errors:
                lines.append(f"{error_type}: {count}íšŒ")
            lines.append("")
        
        # ì‹ ë¢°ë„ ë¶„í¬
        if hasattr(analysis, 'confidence_distribution') and analysis.confidence_distribution:
            lines.append("5. ì‹ ë¢°ë„ ë¶„í¬")
            lines.append("-" * 40)
            for level, count in analysis.confidence_distribution.items():
                lines.append(f"{level}: {count}ê°œ")
            lines.append("")
        
        # ì²˜ë¦¬ ì‹œê°„ í†µê³„
        if hasattr(analysis, 'processing_time_stats') and analysis.processing_time_stats:
            lines.append("6. ì²˜ë¦¬ ì‹œê°„ í†µê³„")
            lines.append("-" * 40)
            stats = analysis.processing_time_stats
            lines.append(f"ìµœì†Œ: {stats.get('min', 0):.3f}ì´ˆ")
            lines.append(f"ìµœëŒ€: {stats.get('max', 0):.3f}ì´ˆ")
            lines.append(f"í‰ê· : {stats.get('mean', 0):.3f}ì´ˆ")
            lines.append(f"ì¤‘ì•™ê°’: {stats.get('median', 0):.3f}ì´ˆ")
            lines.append("")
        
        # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ìƒì„¸
        failed_tests = [r for r in analysis.detailed_results if not r.success]
        if failed_tests:
            lines.append("7. ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ìƒì„¸")
            lines.append("-" * 40)
            for i, result in enumerate(failed_tests[:10], 1):  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                lines.append(f"{i}. í…ŒìŠ¤íŠ¸ ID: {result.test_case.id}")
                lines.append(f"   ì…ë ¥: {result.test_case.input_text}")
                lines.append(f"   ì¹´í…Œê³ ë¦¬: {result.test_case.category.value}")
                lines.append(f"   ì˜ˆìƒ ì˜ë„: {result.test_case.expected_intent.value if result.test_case.expected_intent else 'N/A'}")
                lines.append(f"   ê°ì§€ëœ ì˜ë„: {result.detected_intent.value}")
                lines.append(f"   ì˜¤ë¥˜: {result.error_message}")
                lines.append(f"   ì‹ ë¢°ë„: {result.confidence_score:.3f}")
                lines.append(f"   ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.3f}ì´ˆ")
                lines.append("")
            
            if len(failed_tests) > 10:
                lines.append(f"... ë° {len(failed_tests) - 10}ê°œ ì¶”ê°€ ì‹¤íŒ¨ í…ŒìŠ¤íŠ¸")
                lines.append("")
        
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ìƒì„¸ (ì…ë ¥/ì¶œë ¥ í¬í•¨)
        lines.append("8. ëª¨ë“  í…ŒìŠ¤íŠ¸ ìƒì„¸")
        lines.append("-" * 40)
        for i, result in enumerate(analysis.detailed_results, 1):
            success_mark = "ì„±ê³µ" if result.success else "ì‹¤íŒ¨"
            lines.append(f"{i}. í…ŒìŠ¤íŠ¸ ID: {result.test_case.id}")
            lines.append(f"   ì…ë ¥ í…ìŠ¤íŠ¸: {result.test_case.input_text}")
            lines.append(f"   ì¶œë ¥ í…ìŠ¤íŠ¸: {result.system_response}")
            lines.append(f"   ê°ì§€ëœ ì˜ë„: {result.detected_intent.value}")
            lines.append(f"   ì‹ ë¢°ë„: {result.confidence_score:.3f}")
            lines.append(f"   ì„±ê³µ ì—¬ë¶€: {success_mark}")
            lines.append(f"   ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.3f}ì´ˆ")
            lines.append("")
        
        lines.append("=" * 80)
        lines.append("ë³´ê³ ì„œ ë")
        lines.append("=" * 80)
        
        return "\n".join(lines)
    
    def _generate_markdown_content(self, analysis: TestAnalysis) -> str:
        """ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ë‚´ìš© ìƒì„±"""
        lines = []
        
        # í—¤ë”
        lines.append("# ìŒì„± í‚¤ì˜¤ìŠ¤í¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë³´ê³ ì„œ")
        lines.append("")
        lines.append(f"**ìƒì„± ì‹œê°„:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")
        
        # ì „ì²´ ìš”ì•½
        lines.append("## 1. ì „ì²´ ìš”ì•½")
        lines.append("")
        lines.append("| í•­ëª© | ê°’ |")
        lines.append("|------|-----|")
        lines.append(f"| ì´ í…ŒìŠ¤íŠ¸ ìˆ˜ | {analysis.total_tests} |")
        lines.append(f"| ì„±ê³µë¥  | {analysis.success_rate:.2%} |")
        lines.append(f"| í‰ê·  ì²˜ë¦¬ ì‹œê°„ | {analysis.average_processing_time:.3f}ì´ˆ |")
        lines.append(f"| ì´ ì˜¤ë¥˜ ìˆ˜ | {sum(analysis.error_summary.values())} |")
        lines.append("")
        
        # ì˜ë„ë³„ ì •í™•ë„
        if analysis.intent_accuracy:
            lines.append("## 2. ì˜ë„ë³„ ì •í™•ë„")
            lines.append("")
            lines.append("| ì˜ë„ | ì •í™•ë„ |")
            lines.append("|------|--------|")
            for intent, accuracy in sorted(analysis.intent_accuracy.items()):
                lines.append(f"| {intent} | {accuracy:.2%} |")
            lines.append("")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥
        if analysis.category_performance:
            lines.append("## 3. ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥")
            lines.append("")
            lines.append("| ì¹´í…Œê³ ë¦¬ | ì„±ê³µë¥  |")
            lines.append("|----------|--------|")
            for category, performance in sorted(analysis.category_performance.items()):
                lines.append(f"| {category} | {performance:.2%} |")
            lines.append("")
        
        # ì˜¤ë¥˜ ë¶„ì„
        if analysis.error_summary:
            lines.append("## 4. ì˜¤ë¥˜ ë¶„ì„")
            lines.append("")
            lines.append("| ì˜¤ë¥˜ ìœ í˜• | ë°œìƒ íšŸìˆ˜ |")
            lines.append("|-----------|-----------|")
            sorted_errors = sorted(analysis.error_summary.items(), key=lambda x: x[1], reverse=True)
            for error_type, count in sorted_errors:
                lines.append(f"| {error_type} | {count} |")
            lines.append("")
        
        # ì‹ ë¢°ë„ ë¶„í¬
        if hasattr(analysis, 'confidence_distribution') and analysis.confidence_distribution:
            lines.append("## 5. ì‹ ë¢°ë„ ë¶„í¬")
            lines.append("")
            lines.append("| ì‹ ë¢°ë„ ìˆ˜ì¤€ | ê°œìˆ˜ |")
            lines.append("|-------------|------|")
            for level, count in analysis.confidence_distribution.items():
                lines.append(f"| {level} | {count} |")
            lines.append("")
        
        # ì²˜ë¦¬ ì‹œê°„ í†µê³„
        if hasattr(analysis, 'processing_time_stats') and analysis.processing_time_stats:
            lines.append("## 6. ì²˜ë¦¬ ì‹œê°„ í†µê³„")
            lines.append("")
            stats = analysis.processing_time_stats
            lines.append("| í†µê³„ | ê°’ |")
            lines.append("|------|-----|")
            lines.append(f"| ìµœì†Œ | {stats.get('min', 0):.3f}ì´ˆ |")
            lines.append(f"| ìµœëŒ€ | {stats.get('max', 0):.3f}ì´ˆ |")
            lines.append(f"| í‰ê·  | {stats.get('mean', 0):.3f}ì´ˆ |")
            lines.append(f"| ì¤‘ì•™ê°’ | {stats.get('median', 0):.3f}ì´ˆ |")
            lines.append("")
        
        # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ìƒì„¸
        failed_tests = [r for r in analysis.detailed_results if not r.success]
        if failed_tests:
            lines.append("## 7. ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ìƒì„¸")
            lines.append("")
            for i, result in enumerate(failed_tests[:10], 1):  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                lines.append(f"### ì‹¤íŒ¨ í…ŒìŠ¤íŠ¸ {i}")
                lines.append("")
                lines.append(f"- **í…ŒìŠ¤íŠ¸ ID:** {result.test_case.id}")
                lines.append(f"- **ì…ë ¥:** {result.test_case.input_text}")
                lines.append(f"- **ì¹´í…Œê³ ë¦¬:** {result.test_case.category.value}")
                lines.append(f"- **ì˜ˆìƒ ì˜ë„:** {result.test_case.expected_intent.value if result.test_case.expected_intent else 'N/A'}")
                lines.append(f"- **ê°ì§€ëœ ì˜ë„:** {result.detected_intent.value}")
                lines.append(f"- **ì˜¤ë¥˜:** {result.error_message}")
                lines.append(f"- **ì‹ ë¢°ë„:** {result.confidence_score:.3f}")
                lines.append(f"- **ì²˜ë¦¬ ì‹œê°„:** {result.processing_time:.3f}ì´ˆ")
                lines.append("")
            
            if len(failed_tests) > 10:
                lines.append(f"*... ë° {len(failed_tests) - 10}ê°œ ì¶”ê°€ ì‹¤íŒ¨ í…ŒìŠ¤íŠ¸*")
                lines.append("")
        
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ìƒì„¸ (ì…ë ¥/ì¶œë ¥ í¬í•¨)
        lines.append("## 8. ëª¨ë“  í…ŒìŠ¤íŠ¸ ìƒì„¸")
        lines.append("")
        lines.append("| í…ŒìŠ¤íŠ¸ ID | ì…ë ¥ í…ìŠ¤íŠ¸ | ì¶œë ¥ í…ìŠ¤íŠ¸ | ì˜ë„ | ì‹ ë¢°ë„ | ì„±ê³µ | ì²˜ë¦¬ì‹œê°„ |")
        lines.append("|-----------|-------------|-------------|------|--------|------|----------|")
        
        for result in analysis.detailed_results:
            success_mark = "âœ…" if result.success else "âŒ"
            # í…ìŠ¤íŠ¸ ì „ì²´ í‘œì‹œ (ìë¥´ì§€ ì•ŠìŒ)
            input_text = result.test_case.input_text
            output_text = result.system_response
            lines.append(f"| {result.test_case.id} | {input_text} | {output_text} | {result.detected_intent.value} | {result.confidence_score:.3f} | {success_mark} | {result.processing_time:.3f}ì´ˆ |")
        
        lines.append("")
        
        # ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸
        lines.append("## 9. ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸")
        lines.append("")
        insights = self._generate_performance_insights(analysis)
        for insight in insights:
            lines.append(f"- {insight}")
        lines.append("")
        
        return "\n".join(lines)
    
    def _generate_performance_insights(self, analysis: TestAnalysis) -> List[str]:
        """ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # ì „ì²´ ì„±ê³µë¥  í‰ê°€
        if analysis.success_rate >= 0.9:
            insights.append("âœ… ì „ì²´ ì„±ê³µë¥ ì´ 90% ì´ìƒìœ¼ë¡œ ìš°ìˆ˜í•©ë‹ˆë‹¤.")
        elif analysis.success_rate >= 0.8:
            insights.append("âš ï¸ ì „ì²´ ì„±ê³µë¥ ì´ 80-90%ë¡œ ì–‘í˜¸í•˜ì§€ë§Œ ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
        else:
            insights.append("âŒ ì „ì²´ ì„±ê³µë¥ ì´ 80% ë¯¸ë§Œìœ¼ë¡œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì²˜ë¦¬ ì‹œê°„ í‰ê°€
        if analysis.average_processing_time <= 1.0:
            insights.append("âœ… í‰ê·  ì²˜ë¦¬ ì‹œê°„ì´ 1ì´ˆ ì´í•˜ë¡œ ë¹ ë¦…ë‹ˆë‹¤.")
        elif analysis.average_processing_time <= 3.0:
            insights.append("âš ï¸ í‰ê·  ì²˜ë¦¬ ì‹œê°„ì´ 1-3ì´ˆë¡œ ë³´í†µì…ë‹ˆë‹¤.")
        else:
            insights.append("âŒ í‰ê·  ì²˜ë¦¬ ì‹œê°„ì´ 3ì´ˆ ì´ìƒìœ¼ë¡œ ëŠë¦½ë‹ˆë‹¤.")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„
        if analysis.category_performance:
            worst_category = min(analysis.category_performance.items(), key=lambda x: x[1])
            best_category = max(analysis.category_performance.items(), key=lambda x: x[1])
            
            insights.append(f"ğŸ“Š ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ì¹´í…Œê³ ë¦¬: {best_category[0]} ({best_category[1]:.1%})")
            insights.append(f"ğŸ“Š ê°€ì¥ ì„±ëŠ¥ì´ ë‚˜ìœ ì¹´í…Œê³ ë¦¬: {worst_category[0]} ({worst_category[1]:.1%})")
        
        # ì˜¤ë¥˜ ë¶„ì„
        if analysis.error_summary:
            most_common_error = max(analysis.error_summary.items(), key=lambda x: x[1])
            insights.append(f"ğŸ” ê°€ì¥ ë¹ˆë²ˆí•œ ì˜¤ë¥˜: {most_common_error[0]} ({most_common_error[1]}íšŒ)")
        
        return insights
    
    def _format_statistics(self, analysis: TestAnalysis) -> str:
        """í†µê³„ ë°ì´í„° í¬ë§·íŒ…"""
        stats = []
        stats.append(f"ì´ í…ŒìŠ¤íŠ¸: {analysis.total_tests}")
        stats.append(f"ì„±ê³µë¥ : {analysis.success_rate:.2%}")
        stats.append(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {analysis.average_processing_time:.3f}ì´ˆ")
        
        if analysis.error_summary:
            total_errors = sum(analysis.error_summary.values())
            stats.append(f"ì´ ì˜¤ë¥˜: {total_errors}")
        
        return " | ".join(stats)
    
    def _format_error_details(self, analysis: TestAnalysis) -> str:
        """ì˜¤ë¥˜ ìƒì„¸ ì •ë³´ í¬ë§·íŒ…"""
        if not analysis.error_summary:
            return "ì˜¤ë¥˜ ì—†ìŒ"
        
        error_details = []
        sorted_errors = sorted(analysis.error_summary.items(), key=lambda x: x[1], reverse=True)
        
        for error_type, count in sorted_errors:
            error_details.append(f"{error_type}: {count}íšŒ")
        
        return ", ".join(error_details)