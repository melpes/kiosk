"""
í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ê¸°
"""

from typing import Dict, List, Any
from collections import defaultdict, Counter
from datetime import datetime

from ..models.testing_models import (
    TestResults, TestResult, TestAnalysis, TestCaseCategory
)
from ..models.conversation_models import IntentType
from ..logger import get_logger


class ResultAnalyzer:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
    
    def analyze_results(self, test_results: TestResults) -> TestAnalysis:
        """
        í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„
        
        Args:
            test_results: ë¶„ì„í•  í…ŒìŠ¤íŠ¸ ê²°ê³¼
            
        Returns:
            TestAnalysis: ë¶„ì„ëœ ê²°ê³¼
        """
        try:
            self.logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ì‹œì‘: {test_results.total_tests}ê°œ ê²°ê³¼")
            
            # ê¸°ë³¸ í†µê³„ ê³„ì‚°
            total_tests = test_results.total_tests
            success_rate = test_results.success_rate
            average_processing_time = test_results.average_processing_time
            
            # ì˜ë„ë³„ ì •í™•ë„ ë¶„ì„
            intent_accuracy = self._analyze_intent_accuracy(test_results.results)
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„
            category_performance = self._analyze_category_performance(test_results.results)
            
            # ì˜¤ë¥˜ ìš”ì•½ ë¶„ì„
            error_summary = self._analyze_error_summary(test_results.results)
            
            # ë¶„ì„ ê²°ê³¼ ìƒì„±
            analysis = TestAnalysis(
                total_tests=total_tests,
                success_rate=success_rate,
                average_processing_time=average_processing_time,
                intent_accuracy=intent_accuracy,
                category_performance=category_performance,
                error_summary=error_summary,
                detailed_results=test_results.results
            )
            
            self.logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ì™„ë£Œ: ì„±ê³µë¥  {success_rate:.2%}")
            return analysis
            
        except Exception as e:
            self.logger.error(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ ë°˜í™˜
            return TestAnalysis(
                total_tests=test_results.total_tests,
                success_rate=test_results.success_rate,
                average_processing_time=test_results.average_processing_time,
                intent_accuracy={},
                category_performance={},
                error_summary={},
                detailed_results=test_results.results
            )
    
    def _analyze_intent_accuracy(self, results: List[TestResult]) -> Dict[str, float]:
        """
        ì˜ë„ë³„ ì •í™•ë„ ë¶„ì„
        
        Args:
            results: í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict[str, float]: ì˜ë„ë³„ ì •í™•ë„
        """
        intent_stats = defaultdict(lambda: {'total': 0, 'correct': 0})
        
        for result in results:
            if result.test_case.expected_intent is not None:
                intent_name = result.test_case.expected_intent.value
                intent_stats[intent_name]['total'] += 1
                
                if result.intent_matches:
                    intent_stats[intent_name]['correct'] += 1
        
        # ì •í™•ë„ ê³„ì‚°
        accuracy = {}
        for intent, stats in intent_stats.items():
            if stats['total'] > 0:
                accuracy[intent] = stats['correct'] / stats['total']
            else:
                accuracy[intent] = 0.0
        
        return accuracy
    
    def _analyze_category_performance(self, results: List[TestResult]) -> Dict[str, float]:
        """
        ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„
        
        Args:
            results: í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict[str, float]: ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³µë¥ 
        """
        category_stats = defaultdict(lambda: {'total': 0, 'success': 0})
        
        for result in results:
            category_name = result.test_case.category.value
            category_stats[category_name]['total'] += 1
            
            if result.success:
                category_stats[category_name]['success'] += 1
        
        # ì„±ê³µë¥  ê³„ì‚°
        performance = {}
        for category, stats in category_stats.items():
            if stats['total'] > 0:
                performance[category] = stats['success'] / stats['total']
            else:
                performance[category] = 0.0
        
        return performance
    
    def _analyze_error_summary(self, results: List[TestResult]) -> Dict[str, int]:
        """
        ì˜¤ë¥˜ ìš”ì•½ ë¶„ì„
        
        Args:
            results: í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict[str, int]: ì˜¤ë¥˜ ìœ í˜•ë³„ ê°œìˆ˜
        """
        error_counter = Counter()
        
        for result in results:
            if not result.success and result.error_message:
                # ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ë¶„ë¥˜
                error_type = self._classify_error(result.error_message)
                error_counter[error_type] += 1
        
        return dict(error_counter)
    
    def _classify_error(self, error_message: str) -> str:
        """
        ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ë¶„ë¥˜
        
        Args:
            error_message: ì˜¤ë¥˜ ë©”ì‹œì§€
            
        Returns:
            str: ì˜¤ë¥˜ ìœ í˜•
        """
        error_message_lower = error_message.lower()
        
        if 'timeout' in error_message_lower:
            return 'timeout_error'
        elif 'connection' in error_message_lower:
            return 'connection_error'
        elif 'api' in error_message_lower:
            return 'api_error'
        elif 'intent' in error_message_lower:
            return 'intent_recognition_error'
        elif 'parsing' in error_message_lower or 'parse' in error_message_lower:
            return 'parsing_error'
        elif 'validation' in error_message_lower:
            return 'validation_error'
        else:
            return 'unknown_error'
    
    def generate_summary_statistics(self, analysis: TestAnalysis) -> Dict[str, Any]:
        """
        ìš”ì•½ í†µê³„ ìƒì„±
        
        Args:
            analysis: ë¶„ì„ ê²°ê³¼
            
        Returns:
            Dict[str, Any]: ìš”ì•½ í†µê³„
        """
        return {
            'overview': {
                'total_tests': analysis.total_tests,
                'success_rate': f"{analysis.success_rate:.2%}",
                'average_processing_time': f"{analysis.average_processing_time:.3f}s",
                'total_errors': sum(analysis.error_summary.values())
            },
            'intent_performance': {
                intent: f"{accuracy:.2%}" 
                for intent, accuracy in analysis.intent_accuracy.items()
            },
            'category_performance': {
                category: f"{performance:.2%}" 
                for category, performance in analysis.category_performance.items()
            },
            'confidence_distribution': analysis.confidence_distribution,
            'processing_time_stats': analysis.processing_time_stats,
            'top_errors': dict(Counter(analysis.error_summary).most_common(5))
        }
    
    def get_failed_test_details(self, analysis: TestAnalysis) -> List[Dict[str, Any]]:
        """
        ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ì˜ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        
        Args:
            analysis: ë¶„ì„ ê²°ê³¼
            
        Returns:
            List[Dict[str, Any]]: ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ìƒì„¸ ì •ë³´
        """
        failed_tests = []
        
        for result in analysis.detailed_results:
            if not result.success:
                failed_tests.append({
                    'test_id': result.test_case.id,
                    'input_text': result.test_case.input_text,
                    'category': result.test_case.category.value,
                    'expected_intent': result.test_case.expected_intent.value if result.test_case.expected_intent else None,
                    'detected_intent': result.detected_intent.value,
                    'error_message': result.error_message,
                    'confidence_score': result.confidence_score,
                    'processing_time': result.processing_time
                })
        
        return failed_tests
    
    def get_performance_insights(self, analysis: TestAnalysis) -> List[str]:
        """
        ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸ ìƒì„±
        
        Args:
            analysis: ë¶„ì„ ê²°ê³¼
            
        Returns:
            List[str]: ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸ ëª©ë¡
        """
        insights = []
        
        # ì „ì²´ ì„±ê³µë¥  í‰ê°€
        if analysis.success_rate >= 0.9:
            insights.append("âœ… ì „ì²´ ì„±ê³µë¥ ì´ 90% ì´ìƒìœ¼ë¡œ ìš°ìˆ˜í•©ë‹ˆë‹¤.")
        elif analysis.success_rate >= 0.8:
            insights.append("âš ï¸ ì „ì²´ ì„±ê³µë¥ ì´ 80-90%ë¡œ ì–‘í˜¸í•˜ì§€ë§Œ ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
        else:
            insights.append("âŒ ì „ì²´ ì„±ê³µë¥ ì´ 80% ë¯¸ë§Œìœ¼ë¡œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì²˜ë¦¬ ì‹œê°„ í‰ê°€
        if analysis.average_processing_time <= 1.0:
            insights.append("âœ… í‰ê·  ì²˜ë¦¬ ì‹œê°„ì´ 1ì´ˆ ì´í•˜ë¡œ ë¹ ë¦…ë‹ˆë‹¤.")
        elif analysis.average_processing_time <= 3.0:
            insights.append("âš ï¸ í‰ê·  ì²˜ë¦¬ ì‹œê°„ì´ 1-3ì´ˆë¡œ ë³´í†µì…ë‹ˆë‹¤.")
        else:
            insights.append("âŒ í‰ê·  ì²˜ë¦¬ ì‹œê°„ì´ 3ì´ˆ ì´ìƒìœ¼ë¡œ ëŠë¦½ë‹ˆë‹¤.")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„
        if analysis.category_performance:
            worst_category = min(analysis.category_performance.items(), key=lambda x: x[1])
            best_category = max(analysis.category_performance.items(), key=lambda x: x[1])
            
            insights.append(f"ğŸ“Š ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ì¹´í…Œê³ ë¦¬: {best_category[0]} ({best_category[1]:.1%})")
            insights.append(f"ğŸ“Š ê°€ì¥ ì„±ëŠ¥ì´ ë‚˜ìœ ì¹´í…Œê³ ë¦¬: {worst_category[0]} ({worst_category[1]:.1%})")
        
        # ì˜¤ë¥˜ ë¶„ì„
        if analysis.error_summary:
            most_common_error = max(analysis.error_summary.items(), key=lambda x: x[1])
            insights.append(f"ğŸ” ê°€ì¥ ë¹ˆë²ˆí•œ ì˜¤ë¥˜: {most_common_error[0]} ({most_common_error[1]}íšŒ)")
        
        # ì‹ ë¢°ë„ ë¶„ì„
        if analysis.confidence_distribution:
            low_confidence = analysis.confidence_distribution.get('low', 0)
            total_tests = sum(analysis.confidence_distribution.values())
            if total_tests > 0:
                low_confidence_rate = low_confidence / total_tests
                if low_confidence_rate > 0.2:
                    insights.append(f"âš ï¸ ë‚®ì€ ì‹ ë¢°ë„(0.5 ë¯¸ë§Œ) ê²°ê³¼ê°€ {low_confidence_rate:.1%}ë¡œ ë†’ìŠµë‹ˆë‹¤.")
        
        return insights