#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸
"""

import sys
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
from collections import defaultdict, Counter
from dataclasses import dataclass, field
from enum import Enum

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

# í•„ìš”í•œ ëª¨ë¸ë“¤ì„ ì§ì ‘ ì •ì˜
class IntentType(Enum):
    ORDER = "order"
    MODIFY = "modify"
    CANCEL = "cancel"
    PAYMENT = "payment"
    INQUIRY = "inquiry"
    UNKNOWN = "unknown"

class TestCaseCategory(Enum):
    SLANG = "slang"
    INFORMAL = "informal"
    COMPLEX = "complex"
    NORMAL = "normal"
    EDGE = "edge"

@dataclass
class TestCase:
    id: str
    input_text: str
    expected_intent: IntentType
    category: TestCaseCategory
    description: str
    tags: List[str] = field(default_factory=list)

@dataclass
class TestResult:
    test_case: TestCase
    system_response: str
    detected_intent: IntentType
    processing_time: float
    success: bool
    error_message: str = None
    confidence_score: float = 0.0
    
    @property
    def intent_matches(self) -> bool:
        return self.detected_intent == self.test_case.expected_intent

@dataclass
class TestResults:
    session_id: str
    results: List[TestResult] = field(default_factory=list)
    start_time: datetime = field(default_factory=datetime.now)
    end_time: datetime = None
    
    def add_result(self, result: TestResult):
        self.results.append(result)
    
    def finish(self):
        self.end_time = datetime.now()
    
    @property
    def total_tests(self) -> int:
        return len(self.results)
    
    @property
    def successful_tests(self) -> int:
        return sum(1 for result in self.results if result.success)
    
    @property
    def success_rate(self) -> float:
        if self.total_tests == 0:
            return 0.0
        return self.successful_tests / self.total_tests
    
    @property
    def average_processing_time(self) -> float:
        if not self.results:
            return 0.0
        return sum(result.processing_time for result in self.results) / len(self.results)

@dataclass
class TestAnalysis:
    total_tests: int
    success_rate: float
    average_processing_time: float
    intent_accuracy: Dict[str, float]
    category_performance: Dict[str, float]
    error_summary: Dict[str, int]
    detailed_results: List[TestResult]
    confidence_distribution: Dict[str, int] = field(default_factory=dict)
    processing_time_stats: Dict[str, float] = field(default_factory=dict)
    
    def __post_init__(self):
        if self.detailed_results:
            self._calculate_confidence_distribution()
            self._calculate_processing_time_stats()
    
    def _calculate_confidence_distribution(self):
        ranges = {'very_high': 0, 'high': 0, 'medium': 0, 'low': 0}
        for result in self.detailed_results:
            confidence = result.confidence_score
            if confidence >= 0.9:
                ranges['very_high'] += 1
            elif confidence >= 0.7:
                ranges['high'] += 1
            elif confidence >= 0.5:
                ranges['medium'] += 1
            else:
                ranges['low'] += 1
        self.confidence_distribution = ranges
    
    def _calculate_processing_time_stats(self):
        if not self.detailed_results:
            return
        times = [result.processing_time for result in self.detailed_results]
        self.processing_time_stats = {
            'min': min(times),
            'max': max(times),
            'median': sorted(times)[len(times) // 2],
            'std_dev': self._calculate_std_dev(times)
        }
    
    def _calculate_std_dev(self, values: List[float]) -> float:
        if len(values) < 2:
            return 0.0
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / (len(values) - 1)
        return variance ** 0.5

# ResultAnalyzer í´ë˜ìŠ¤
class ResultAnalyzer:
    def __init__(self):
        pass
    
    def analyze_results(self, test_results: TestResults) -> TestAnalysis:
        print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ì‹œì‘: {test_results.total_tests}ê°œ ê²°ê³¼")
        
        # ê¸°ë³¸ í†µê³„ ê³„ì‚°
        total_tests = test_results.total_tests
        success_rate = test_results.success_rate
        average_processing_time = test_results.average_processing_time
        
        # ì˜ë„ë³„ ì •í™•ë„ ë¶„ì„
        intent_accuracy = self._analyze_intent_accuracy(test_results.results)
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„
        category_performance = self._analyze_category_performance(test_results.results)
        
        # ì˜¤ë¥˜ ìš”ì•½ ë¶„ì„
        error_summary = self._analyze_error_summary(test_results.results)
        
        # ë¶„ì„ ê²°ê³¼ ìƒì„±
        analysis = TestAnalysis(
            total_tests=total_tests,
            success_rate=success_rate,
            average_processing_time=average_processing_time,
            intent_accuracy=intent_accuracy,
            category_performance=category_performance,
            error_summary=error_summary,
            detailed_results=test_results.results
        )
        
        print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ì™„ë£Œ: ì„±ê³µë¥  {success_rate:.2%}")
        return analysis
    
    def _analyze_intent_accuracy(self, results: List[TestResult]) -> Dict[str, float]:
        intent_stats = defaultdict(lambda: {'total': 0, 'correct': 0})
        
        for result in results:
            if result.test_case.expected_intent is not None:
                intent_name = result.test_case.expected_intent.value
                intent_stats[intent_name]['total'] += 1
                
                if result.intent_matches:
                    intent_stats[intent_name]['correct'] += 1
        
        accuracy = {}
        for intent, stats in intent_stats.items():
            if stats['total'] > 0:
                accuracy[intent] = stats['correct'] / stats['total']
            else:
                accuracy[intent] = 0.0
        
        return accuracy
    
    def _analyze_category_performance(self, results: List[TestResult]) -> Dict[str, float]:
        category_stats = defaultdict(lambda: {'total': 0, 'success': 0})
        
        for result in results:
            category_name = result.test_case.category.value
            category_stats[category_name]['total'] += 1
            
            if result.success:
                category_stats[category_name]['success'] += 1
        
        performance = {}
        for category, stats in category_stats.items():
            if stats['total'] > 0:
                performance[category] = stats['success'] / stats['total']
            else:
                performance[category] = 0.0
        
        return performance
    
    def _analyze_error_summary(self, results: List[TestResult]) -> Dict[str, int]:
        error_counter = Counter()
        
        for result in results:
            if not result.success and result.error_message:
                error_type = self._classify_error(result.error_message)
                error_counter[error_type] += 1
        
        return dict(error_counter)
    
    def _classify_error(self, error_message: str) -> str:
        error_message_lower = error_message.lower()
        
        if 'timeout' in error_message_lower:
            return 'timeout_error'
        elif 'connection' in error_message_lower:
            return 'connection_error'
        elif 'api' in error_message_lower:
            return 'api_error'
        elif 'intent' in error_message_lower:
            return 'intent_recognition_error'
        elif 'parsing' in error_message_lower or 'parse' in error_message_lower:
            return 'parsing_error'
        elif 'validation' in error_message_lower:
            return 'validation_error'
        else:
            return 'unknown_error'
    
    def get_performance_insights(self, analysis: TestAnalysis) -> List[str]:
        insights = []
        
        if analysis.success_rate >= 0.9:
            insights.append("âœ… ì „ì²´ ì„±ê³µë¥ ì´ 90% ì´ìƒìœ¼ë¡œ ìš°ìˆ˜í•©ë‹ˆë‹¤.")
        elif analysis.success_rate >= 0.8:
            insights.append("âš ï¸ ì „ì²´ ì„±ê³µë¥ ì´ 80-90%ë¡œ ì–‘í˜¸í•˜ì§€ë§Œ ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
        else:
            insights.append("âŒ ì „ì²´ ì„±ê³µë¥ ì´ 80% ë¯¸ë§Œìœ¼ë¡œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if analysis.average_processing_time <= 1.0:
            insights.append("âœ… í‰ê·  ì²˜ë¦¬ ì‹œê°„ì´ 1ì´ˆ ì´í•˜ë¡œ ë¹ ë¦…ë‹ˆë‹¤.")
        elif analysis.average_processing_time <= 3.0:
            insights.append("âš ï¸ í‰ê·  ì²˜ë¦¬ ì‹œê°„ì´ 1-3ì´ˆë¡œ ë³´í†µì…ë‹ˆë‹¤.")
        else:
            insights.append("âŒ í‰ê·  ì²˜ë¦¬ ì‹œê°„ì´ 3ì´ˆ ì´ìƒìœ¼ë¡œ ëŠë¦½ë‹ˆë‹¤.")
        
        if analysis.category_performance:
            worst_category = min(analysis.category_performance.items(), key=lambda x: x[1])
            best_category = max(analysis.category_performance.items(), key=lambda x: x[1])
            
            insights.append(f"ğŸ“Š ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ì¹´í…Œê³ ë¦¬: {best_category[0]} ({best_category[1]:.1%})")
            insights.append(f"ğŸ“Š ê°€ì¥ ì„±ëŠ¥ì´ ë‚˜ìœ ì¹´í…Œê³ ë¦¬: {worst_category[0]} ({worst_category[1]:.1%})")
        
        if analysis.error_summary:
            most_common_error = max(analysis.error_summary.items(), key=lambda x: x[1])
            insights.append(f"ğŸ” ê°€ì¥ ë¹ˆë²ˆí•œ ì˜¤ë¥˜: {most_common_error[0]} ({most_common_error[1]}íšŒ)")
        
        return insights

# ReportGenerator í´ë˜ìŠ¤
class ReportGenerator:
    def __init__(self, output_directory: str = "test_results"):
        self.output_directory = Path(output_directory)
        self.output_directory.mkdir(parents=True, exist_ok=True)
        self.reports_dir = self.output_directory / "reports"
        self.reports_dir.mkdir(exist_ok=True)
    
    def generate_text_report(self, analysis: TestAnalysis, output_path: str = None) -> str:
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = self.reports_dir / f"test_report_{timestamp}.txt"
        else:
            output_path = Path(output_path)
        
        print(f"í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„± ì‹œì‘: {output_path}")
        
        content = self._generate_text_content(analysis)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {output_path}")
        return str(output_path)
    
    def generate_markdown_report(self, analysis: TestAnalysis, output_path: str = None) -> str:
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = self.reports_dir / f"test_report_{timestamp}.md"
        else:
            output_path = Path(output_path)
        
        print(f"ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„± ì‹œì‘: {output_path}")
        
        content = self._generate_markdown_content(analysis)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {output_path}")
        return str(output_path)
    
    def generate_summary_report(self, analysis: TestAnalysis) -> str:
        lines = []
        lines.append("=" * 60)
        lines.append("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        lines.append("=" * 60)
        lines.append("")
        
        lines.append("ğŸ“Š ê¸°ë³¸ í†µê³„")
        lines.append(f"  â€¢ ì „ì²´ í…ŒìŠ¤íŠ¸: {analysis.total_tests}ê°œ")
        lines.append(f"  â€¢ ì„±ê³µë¥ : {analysis.success_rate:.2%}")
        lines.append(f"  â€¢ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {analysis.average_processing_time:.3f}ì´ˆ")
        lines.append("")
        
        if analysis.category_performance:
            lines.append("ğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥")
            for category, performance in sorted(analysis.category_performance.items()):
                lines.append(f"  â€¢ {category}: {performance:.2%}")
            lines.append("")
        
        if analysis.error_summary:
            lines.append("âŒ ì£¼ìš” ì˜¤ë¥˜")
            sorted_errors = sorted(analysis.error_summary.items(), key=lambda x: x[1], reverse=True)
            for error_type, count in sorted_errors[:3]:
                lines.append(f"  â€¢ {error_type}: {count}íšŒ")
            lines.append("")
        
        lines.append("=" * 60)
        
        return "\n".join(lines)
    
    def _generate_text_content(self, analysis: TestAnalysis) -> str:
        lines = []
        
        lines.append("=" * 80)
        lines.append("ìŒì„± í‚¤ì˜¤ìŠ¤í¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë³´ê³ ì„œ")
        lines.append("=" * 80)
        lines.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")
        
        lines.append("1. ì „ì²´ ìš”ì•½")
        lines.append("-" * 40)
        lines.append(f"ì´ í…ŒìŠ¤íŠ¸ ìˆ˜: {analysis.total_tests}")
        lines.append(f"ì„±ê³µë¥ : {analysis.success_rate:.2%}")
        lines.append(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {analysis.average_processing_time:.3f}ì´ˆ")
        lines.append(f"ì´ ì˜¤ë¥˜ ìˆ˜: {sum(analysis.error_summary.values())}")
        lines.append("")
        
        if analysis.intent_accuracy:
            lines.append("2. ì˜ë„ë³„ ì •í™•ë„")
            lines.append("-" * 40)
            for intent, accuracy in sorted(analysis.intent_accuracy.items()):
                lines.append(f"{intent}: {accuracy:.2%}")
            lines.append("")
        
        if analysis.category_performance:
            lines.append("3. ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥")
            lines.append("-" * 40)
            for category, performance in sorted(analysis.category_performance.items()):
                lines.append(f"{category}: {performance:.2%}")
            lines.append("")
        
        if analysis.error_summary:
            lines.append("4. ì˜¤ë¥˜ ë¶„ì„")
            lines.append("-" * 40)
            sorted_errors = sorted(analysis.error_summary.items(), key=lambda x: x[1], reverse=True)
            for error_type, count in sorted_errors:
                lines.append(f"{error_type}: {count}íšŒ")
            lines.append("")
        
        lines.append("=" * 80)
        lines.append("ë³´ê³ ì„œ ë")
        lines.append("=" * 80)
        
        return "\n".join(lines)
    
    def _generate_markdown_content(self, analysis: TestAnalysis) -> str:
        lines = []
        
        lines.append("# ìŒì„± í‚¤ì˜¤ìŠ¤í¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë³´ê³ ì„œ")
        lines.append("")
        lines.append(f"**ìƒì„± ì‹œê°„:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")
        
        lines.append("## 1. ì „ì²´ ìš”ì•½")
        lines.append("")
        lines.append("| í•­ëª© | ê°’ |")
        lines.append("|------|-----|")
        lines.append(f"| ì´ í…ŒìŠ¤íŠ¸ ìˆ˜ | {analysis.total_tests} |")
        lines.append(f"| ì„±ê³µë¥  | {analysis.success_rate:.2%} |")
        lines.append(f"| í‰ê·  ì²˜ë¦¬ ì‹œê°„ | {analysis.average_processing_time:.3f}ì´ˆ |")
        lines.append(f"| ì´ ì˜¤ë¥˜ ìˆ˜ | {sum(analysis.error_summary.values())} |")
        lines.append("")
        
        if analysis.intent_accuracy:
            lines.append("## 2. ì˜ë„ë³„ ì •í™•ë„")
            lines.append("")
            lines.append("| ì˜ë„ | ì •í™•ë„ |")
            lines.append("|------|--------|")
            for intent, accuracy in sorted(analysis.intent_accuracy.items()):
                lines.append(f"| {intent} | {accuracy:.2%} |")
            lines.append("")
        
        if analysis.category_performance:
            lines.append("## 3. ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥")
            lines.append("")
            lines.append("| ì¹´í…Œê³ ë¦¬ | ì„±ê³µë¥  |")
            lines.append("|----------|--------|")
            for category, performance in sorted(analysis.category_performance.items()):
                lines.append(f"| {category} | {performance:.2%} |")
            lines.append("")
        
        return "\n".join(lines)

def create_sample_test_results() -> TestResults:
    """ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒì„±"""
    test_results = TestResults(session_id="test_session_001")
    
    # ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ë“¤ ìƒì„±
    test_cases = [
        TestCase(
            id="test_001",
            input_text="ë¹…ë§¥ ì„¸íŠ¸ í•˜ë‚˜ ì£¼ì„¸ìš”",
            expected_intent=IntentType.ORDER,
            category=TestCaseCategory.NORMAL,
            description="ì¼ë°˜ì ì¸ ì£¼ë¬¸"
        ),
        TestCase(
            id="test_002", 
            input_text="ìƒìŠ¤ì¹˜ì½¤ í•˜ë‚˜",
            expected_intent=IntentType.ORDER,
            category=TestCaseCategory.SLANG,
            description="ì€ì–´ ì‚¬ìš© ì£¼ë¬¸"
        ),
        TestCase(
            id="test_003",
            input_text="ë¹…ë§¥ ë¹¼ê³  ì‹¶ì–´",
            expected_intent=IntentType.CANCEL,
            category=TestCaseCategory.INFORMAL,
            description="ë°˜ë§ ì·¨ì†Œ ìš”ì²­"
        ),
        TestCase(
            id="test_004",
            input_text="ë­ê°€ ë§›ìˆì–´?",
            expected_intent=IntentType.INQUIRY,
            category=TestCaseCategory.NORMAL,
            description="ë©”ë‰´ ë¬¸ì˜"
        ),
        TestCase(
            id="test_005",
            input_text="ë² í† ë”” ì¶”ê°€í•˜ê³  ì½œë¼ëŠ” ë¹¼ì¤˜",
            expected_intent=IntentType.MODIFY,
            category=TestCaseCategory.COMPLEX,
            description="ë³µí•© ì˜ë„ - ì¶”ê°€ì™€ ì œê±°"
        )
    ]
    
    # í…ŒìŠ¤íŠ¸ ê²°ê³¼ë“¤ ìƒì„±
    results = [
        TestResult(
            test_case=test_cases[0],
            system_response="ë¹…ë§¥ ì„¸íŠ¸ 1ê°œë¥¼ ì£¼ë¬¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
            detected_intent=IntentType.ORDER,
            processing_time=0.85,
            success=True,
            confidence_score=0.95
        ),
        TestResult(
            test_case=test_cases[1],
            system_response="ìƒí•˜ì´ ìŠ¤íŒŒì´ì‹œ ì¹˜í‚¨ ë²„ê±° ì½¤ë³´ 1ê°œë¥¼ ì£¼ë¬¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
            detected_intent=IntentType.ORDER,
            processing_time=1.2,
            success=True,
            confidence_score=0.88
        ),
        TestResult(
            test_case=test_cases[2],
            system_response="ë¹…ë§¥ì„ ì£¼ë¬¸ì—ì„œ ì œê±°í•˜ê² ìŠµë‹ˆë‹¤.",
            detected_intent=IntentType.CANCEL,
            processing_time=0.75,
            success=True,
            confidence_score=0.92
        ),
        TestResult(
            test_case=test_cases[3],
            system_response="ì£„ì†¡í•©ë‹ˆë‹¤. ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            detected_intent=IntentType.UNKNOWN,
            processing_time=2.1,
            success=False,
            confidence_score=0.35,
            error_message="intent_recognition_error: ì˜ë„ íŒŒì•… ì‹¤íŒ¨"
        ),
        TestResult(
            test_case=test_cases[4],
            system_response="ì£¼ë¬¸ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            detected_intent=IntentType.ORDER,
            processing_time=1.8,
            success=False,
            confidence_score=0.45,
            error_message="parsing_error: ë³µí•© ì˜ë„ ì²˜ë¦¬ ì‹¤íŒ¨"
        )
    ]
    
    for result in results:
        test_results.add_result(result)
    
    test_results.finish()
    return test_results

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„± ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    try:
        # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        test_results = create_sample_test_results()
        print(f"ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒì„± ì™„ë£Œ: {test_results.total_tests}ê°œ")
        
        # ResultAnalyzer í…ŒìŠ¤íŠ¸
        print("\n" + "=" * 60)
        print("ResultAnalyzer í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 60)
        
        analyzer = ResultAnalyzer()
        analysis = analyzer.analyze_results(test_results)
        
        print(f"\në¶„ì„ ê²°ê³¼:")
        print(f"- ì „ì²´ í…ŒìŠ¤íŠ¸: {analysis.total_tests}")
        print(f"- ì„±ê³µë¥ : {analysis.success_rate:.2%}")
        print(f"- í‰ê·  ì²˜ë¦¬ ì‹œê°„: {analysis.average_processing_time:.3f}ì´ˆ")
        
        print(f"\nì˜ë„ë³„ ì •í™•ë„:")
        for intent, accuracy in analysis.intent_accuracy.items():
            print(f"- {intent}: {accuracy:.2%}")
        
        print(f"\nì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥:")
        for category, performance in analysis.category_performance.items():
            print(f"- {category}: {performance:.2%}")
        
        print(f"\nì˜¤ë¥˜ ìš”ì•½:")
        for error_type, count in analysis.error_summary.items():
            print(f"- {error_type}: {count}íšŒ")
        
        print(f"\nì‹ ë¢°ë„ ë¶„í¬:")
        for level, count in analysis.confidence_distribution.items():
            print(f"- {level}: {count}ê°œ")
        
        # ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸
        insights = analyzer.get_performance_insights(analysis)
        print(f"\nì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸:")
        for insight in insights:
            print(f"- {insight}")
        
        # ReportGenerator í…ŒìŠ¤íŠ¸
        print("\n" + "=" * 60)
        print("ReportGenerator í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 60)
        
        generator = ReportGenerator(output_directory="test_results")
        
        # ìš”ì•½ ë³´ê³ ì„œ í…ŒìŠ¤íŠ¸
        print("\nìš”ì•½ ë³´ê³ ì„œ:")
        summary = generator.generate_summary_report(analysis)
        print(summary)
        
        # í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±
        text_report_path = generator.generate_text_report(analysis)
        
        # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
        markdown_report_path = generator.generate_markdown_report(analysis)
        
        # ìƒì„±ëœ íŒŒì¼ë“¤ í™•ì¸
        print(f"\nìƒì„±ëœ íŒŒì¼ë“¤:")
        if os.path.exists(text_report_path):
            file_size = os.path.getsize(text_report_path)
            print(f"- {text_report_path} ({file_size} bytes)")
        
        if os.path.exists(markdown_report_path):
            file_size = os.path.getsize(markdown_report_path)
            print(f"- {markdown_report_path} ({file_size} bytes)")
        
        print("\n" + "=" * 80)
        print("ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("=" * 80)
        
        return 0
        
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)