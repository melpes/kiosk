"""
ìŒì„± ì „ì²˜ë¦¬ ëª¨ë“ˆ
"""

import numpy as np
import librosa
import torch
import torchaudio
import os
import tempfile
import shutil
from typing import Optional, Tuple, Dict, Any
import logging
from dataclasses import dataclass
from collections import defaultdict

# ê³ ê¸‰ í™”ì ë¶„ë¦¬ë¥¼ ìœ„í•œ import (ìµœì‹  ë²„ì „ 3.3.2, 1.0.3 ê¸°ì¤€)
try:
    from speechbrain.inference.separation import SepformerSeparation as separator
    from speechbrain.inference.speaker import SpeakerRecognition
except ImportError:
    # êµ¬ë²„ì „ í˜¸í™˜ì„±
    try:
        from speechbrain.pretrained import SepformerSeparation as separator
        from speechbrain.pretrained import SpeakerRecognition
    except ImportError:
        separator = None
        SpeakerRecognition = None

try:
    from pyannote.audio import Pipeline
    from pyannote.audio.pipelines.utils.hook import ProgressHook
except ImportError:
    Pipeline = None
    ProgressHook = None

from ..models.audio_models import AudioData, ProcessedAudio
from ..models.config_models import AudioConfig
from ..models.error_models import AudioError, AudioErrorType


@dataclass
class NoiseReductionConfig:
    """ë…¸ì´ì¦ˆ ì œê±° ì„¤ì •"""
    enabled: bool = True
    reduction_level: float = 0.5  # 0.0 ~ 1.0
    spectral_gating: bool = True
    stationary_noise_reduction: bool = True


@dataclass
class SpeakerSeparationConfig:
    """í™”ì ë¶„ë¦¬ ì„¤ì •"""
    enabled: bool = True
    threshold: float = 0.7  # í™”ì ë¶„ë¦¬ ì„ê³„ê°’
    max_speakers: int = 5   # ìµœëŒ€ í™”ì ìˆ˜
    target_speaker_selection: str = "loudest"  # "loudest", "first", "manual"
    
    # ê³ ê¸‰ í™”ì ë¶„ë¦¬ ì„¤ì •
    use_advanced_separation: bool = True  # ê³ ê¸‰ ë¶„ë¦¬ ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€
    huggingface_token: Optional[str] = None  # HuggingFace í† í°
    diarization_enabled: bool = True  # Diarization ì‚¬ìš© ì—¬ë¶€
    sepformer_enabled: bool = True  # SepFormer ì‚¬ìš© ì—¬ë¶€
    speaker_recognition_enabled: bool = True  # í™”ì ì¸ì‹ ì‚¬ìš© ì—¬ë¶€
    fallback_to_simple: bool = True  # ê³ ê¸‰ ëª¨ë¸ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©
    min_segment_length: float = 0.5  # ìµœì†Œ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)
    energy_threshold_ratio: float = 0.7  # ì—ë„ˆì§€ ì„ê³„ê°’ ë¹„ìœ¨
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ í™˜ê²½ë³€ìˆ˜ì—ì„œ í† í° ìë™ ë¡œë“œ"""
        if self.huggingface_token is None:
            import os
            # í™˜ê²½ë³€ìˆ˜ì—ì„œ í† í° ë¡œë“œ ì‹œë„
            self.huggingface_token = os.getenv('HUGGINGFACE_TOKEN') or os.getenv('HF_TOKEN')


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def rms_safe_normalize(waveform: torch.Tensor, max_target_rms: float = 0.05) -> torch.Tensor:
    """
    ì•ˆì „í•œ RMS ì •ê·œí™”
    
    Args:
        waveform: ì…ë ¥ íŒŒí˜• í…ì„œ
        max_target_rms: ìµœëŒ€ ëª©í‘œ RMS ê°’
        
    Returns:
        ì •ê·œí™”ëœ íŒŒí˜•
        
    Raises:
        ValueError: ì˜¤ë””ì˜¤ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°
    """
    rms = (waveform ** 2).mean().sqrt()
    peak = waveform.abs().max()

    if rms == 0 or peak == 0:
        raise ValueError('ì˜¤ë””ì˜¤íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤')

    safe_target_rms = min(rms * (1.0 / peak), max_target_rms)
    scaled = waveform * (safe_target_rms / rms)
    return scaled.float()


class AudioProcessor:
    """ìŒì„± ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config: AudioConfig):
        """
        AudioProcessor ì´ˆê¸°í™”
        
        Args:
            config: ìŒì„± ì²˜ë¦¬ ì„¤ì •
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # ë…¸ì´ì¦ˆ ì œê±° ì„¤ì •
        self.noise_config = NoiseReductionConfig(
            enabled=getattr(config, 'noise_reduction_enabled', True),
            reduction_level=getattr(config, 'noise_reduction_level', 0.5)
        )
        
        # í™”ì ë¶„ë¦¬ ì„¤ì •
        self.speaker_config = SpeakerSeparationConfig(
            enabled=getattr(config, 'speaker_separation_enabled', True),
            threshold=getattr(config, 'speaker_separation_threshold', 0.7)
        )
        
        # ëª¨ë¸ ìºì‹±ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.models_cache_dir = os.path.join(tempfile.gettempdir(), "speechbrain_models")
        os.makedirs(self.models_cache_dir, exist_ok=True)
        
        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìºì‹±
        self._sepformer_model = None
        self._speaker_recognition_model = None
        self._diarization_pipeline = None
        
        self.logger.info("AudioProcessor ì´ˆê¸°í™” ì™„ë£Œ")
    
    def process_audio(self, audio_input: AudioData) -> ProcessedAudio:
        """
        ìŒì„± ë°ì´í„° ì „ì²˜ë¦¬
        
        Args:
            audio_input: ì›ë³¸ ìŒì„± ë°ì´í„°
            
        Returns:
            ì „ì²˜ë¦¬ëœ ìŒì„± ë°ì´í„°
            
        Raises:
            AudioError: ìŒì„± ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ
        """
        try:
            self.logger.info(f"ìŒì„± ì²˜ë¦¬ ì‹œì‘ - ê¸¸ì´: {audio_input.duration}ì´ˆ, ìƒ˜í”Œë ˆì´íŠ¸: {audio_input.sample_rate}Hz")
            
            # 1. ì…ë ¥ ê²€ì¦
            self._validate_input(audio_input)
            
            # 2. ë‹¤ì¤‘í™”ì ë¶„ë¦¬ (í™œì„±í™”ëœ ê²½ìš°)
            processed_audio = audio_input.data
            if self.speaker_config.enabled:
                processed_audio = self._separate_speakers(processed_audio, audio_input.sample_rate)
            
            # 3. ë…¸ì´ì¦ˆ ì œê±° (í™œì„±í™”ëœ ê²½ìš°)
            if self.noise_config.enabled:
                processed_audio = self._reduce_noise(processed_audio, audio_input.sample_rate)
            
            # 4. 16kHzë¡œ ë¦¬ìƒ˜í”Œë§
            if audio_input.sample_rate != 16000:
                processed_audio = librosa.resample(
                    processed_audio, 
                    orig_sr=audio_input.sample_rate, 
                    target_sr=16000
                )
            
            # 5. Log-Mel spectrogram íŠ¹ì§• ì¶”ì¶œ
            features = self._extract_mel_features(processed_audio)
            
            result = ProcessedAudio(
                features=features,
                sample_rate=16000,
                original_duration=audio_input.duration
            )
            
            self.logger.info(f"ìŒì„± ì²˜ë¦¬ ì™„ë£Œ - íŠ¹ì§• í¬ê¸°: {features.shape}")
            return result
            
        except Exception as e:
            self.logger.error(f"ìŒì„± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise AudioError(
                error_type=AudioErrorType.PROCESSING_FAILED,
                message=f"ìŒì„± ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
                details={'original_error': str(e)}
            )
    
    def _validate_input(self, audio_input: AudioData):
        """ì…ë ¥ ìŒì„± ë°ì´í„° ê²€ì¦"""
        if audio_input.duration > 30.0:
            raise AudioError(
                error_type=AudioErrorType.INVALID_FORMAT,
                message="ìŒì„± ê¸¸ì´ê°€ 30ì´ˆë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤",
                details={'duration': audio_input.duration}
            )
        
        if audio_input.sample_rate < 8000:
            raise AudioError(
                error_type=AudioErrorType.INVALID_FORMAT,
                message="ìƒ˜í”Œë§ ë ˆì´íŠ¸ê°€ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤ (ìµœì†Œ 8kHz)",
                details={'sample_rate': audio_input.sample_rate}
            )
    
    def _separate_speakers(self, audio_data: np.ndarray, sample_rate: int) -> np.ndarray:
        """
        ê³ ê¸‰ ë‹¤ì¤‘í™”ì ë¶„ë¦¬ ì²˜ë¦¬
        
        Args:
            audio_data: ìŒì„± ë°ì´í„°
            sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸
            
        Returns:
            ì£¼ í™”ì ìŒì„± ë°ì´í„°
        """
        try:
            print("\n" + "="*60)
            print("ğŸ¤ ê³ ê¸‰ ë‹¤ì¤‘í™”ì ë¶„ë¦¬ ì‹œìŠ¤í…œ ì‹œì‘")
            print("="*60)
            
            # ê³ ê¸‰ ë¶„ë¦¬ ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
            if (self.speaker_config.use_advanced_separation and 
                Pipeline is not None and 
                separator is not None and 
                SpeakerRecognition is not None):
                
                print("âœ… ê³ ê¸‰ AI ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ - Pyannote + SepFormer + ECAPA í™œì„±í™”")
                result = self._advanced_speaker_separation(audio_data, sample_rate)
                print("ğŸ¯ ê³ ê¸‰ í™”ì ë¶„ë¦¬ ì™„ë£Œ!")
                return result
            else:
                print("âš ï¸  ê³ ê¸‰ ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€, ê¸°ë³¸ ì—ë„ˆì§€ ê¸°ë°˜ ë¶„ë¦¬ ì‚¬ìš©")
                missing_models = []
                if Pipeline is None:
                    missing_models.append("Pyannote")
                if separator is None:
                    missing_models.append("SepFormer")
                if SpeakerRecognition is None:
                    missing_models.append("ECAPA")
                if missing_models:
                    print(f"âŒ ëˆ„ë½ëœ ëª¨ë¸: {', '.join(missing_models)}")
                
                print("ğŸ”„ ê¸°ë³¸ ì—ë„ˆì§€ ê¸°ë°˜ ë¶„ë¦¬ë¡œ ì „í™˜í•©ë‹ˆë‹¤...")
                result = self._simple_speaker_separation(audio_data, sample_rate)
                print("âœ… ê¸°ë³¸ ì—ë„ˆì§€ ê¸°ë°˜ ë¶„ë¦¬ ì™„ë£Œ")
                return result
                
        except Exception as e:
            print(f"\nâŒ í™”ì ë¶„ë¦¬ ì‹¤íŒ¨: {str(e)}")
            if self.speaker_config.fallback_to_simple:
                print("ğŸ”„ ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´ ì‹œë„...")
                result = self._simple_speaker_separation(audio_data, sample_rate)
                print("âœ… ê¸°ë³¸ ë°©ì‹ ëŒ€ì²´ ì™„ë£Œ")
                return result
            else:
                print("âš ï¸  Fallback ë¹„í™œì„±í™” - ì›ë³¸ ë°˜í™˜")
                return audio_data
    
    def _advanced_speaker_separation(self, audio_data: np.ndarray, sample_rate: int) -> np.ndarray:
        """
        Pyannote + SepFormer + ECAPA ê¸°ë°˜ ê³ ê¸‰ í™”ì ë¶„ë¦¬
        
        Args:
            audio_data: ìŒì„± ë°ì´í„° (numpy array)
            sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸
            
        Returns:
            ì£¼ í™”ì ìŒì„± ë°ì´í„°
        """
        try:
            print("\nğŸš€ ê³ ê¸‰ í™”ì ë¶„ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
            print(f"ğŸ“Š ì…ë ¥: {len(audio_data)} ìƒ˜í”Œ, {sample_rate}Hz")
            
            # 1. ì„ì‹œ íŒŒì¼ ìƒì„± (pyannoteëŠ” íŒŒì¼ ê²½ë¡œê°€ í•„ìš”)
            temp_dir = tempfile.mkdtemp(prefix="speaker_separation_")
            temp_file = os.path.join(temp_dir, 'temp_audio_for_diarization.wav')
            
            # ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ì˜¬ë°”ë¥¸ ë²”ìœ„ì— ìˆëŠ”ì§€ í™•ì¸
            if np.max(np.abs(audio_data)) > 1.0:
                audio_data = audio_data / np.max(np.abs(audio_data)) * 0.95
            
            torchaudio.save(temp_file, torch.from_numpy(audio_data).unsqueeze(0).float(), sample_rate)
            
            try:
                print("\nğŸ¯ 1ë‹¨ê³„: Pyannote Diarization ìˆ˜í–‰ ì¤‘...")
                # 2. Diarization ìˆ˜í–‰
                main_speaker_segments = self._perform_diarization(temp_file, audio_data, sample_rate)
                
                if not main_speaker_segments:
                    print("âŒ Diarizationìœ¼ë¡œ ì£¼ í™”ìë¥¼ ì°¾ì§€ ëª»í•¨ â†’ ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ì „í™˜")
                    print("ğŸ”„ ê¸°ë³¸ ì—ë„ˆì§€ ê¸°ë°˜ ë¶„ë¦¬ë¡œ ì „í™˜í•©ë‹ˆë‹¤...")
                    result = self._simple_speaker_separation(audio_data, sample_rate)
                    print("âœ… ê¸°ë³¸ ë°©ì‹ ëŒ€ì²´ ì™„ë£Œ")
                    return result
                
                print(f"âœ… ì£¼ í™”ì ì„¸ê·¸ë¨¼íŠ¸ {len(main_speaker_segments)}ê°œ ë°œê²¬!")
                
                # 3. SepFormer ìŒì„± ë¶„ë¦¬ (ì„ íƒì )
                if self.speaker_config.sepformer_enabled:
                    print("\nğŸ¯ 2ë‹¨ê³„: SepFormer ìŒì„± ë¶„ë¦¬ ìˆ˜í–‰ ì¤‘...")
                    separated_audio = self._perform_sepformer_separation(audio_data, sample_rate, main_speaker_segments)
                    if separated_audio is not None:
                        print("âœ… SepFormer ë¶„ë¦¬ + ECAPA ë§¤ì¹­ ì„±ê³µ!")
                        return separated_audio
                    else:
                        print("âš ï¸  SepFormer ë¶„ë¦¬ ì‹¤íŒ¨ â†’ Diarization ê²°ê³¼ë§Œ ì‚¬ìš©")
                else:
                    print("âš ï¸  SepFormer ë¹„í™œì„±í™” â†’ Diarization ê²°ê³¼ë§Œ ì‚¬ìš©")
                
                # 4. Diarization ê²°ê³¼ë§Œìœ¼ë¡œ ì£¼ í™”ì ì¶”ì¶œ
                print("\nğŸ¯ 3ë‹¨ê³„: Diarization ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ")
                result = self._extract_main_speaker_from_segments(audio_data, sample_rate, main_speaker_segments)
                print("âœ… Diarization ê¸°ë°˜ ì£¼ í™”ì ì¶”ì¶œ ì™„ë£Œ!")
                return result
                
            finally:
                # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
                if os.path.exists(temp_dir):
                    shutil.rmtree(temp_dir, ignore_errors=True)
                    
        except Exception as e:
            self.logger.error(f"ê³ ê¸‰ í™”ì ë¶„ë¦¬ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _perform_diarization(self, audio_file: str, audio_data: np.ndarray, sample_rate: int) -> Optional[list]:
        """
        Pyannoteë¥¼ ì‚¬ìš©í•œ í™”ì êµ¬ë¶„ ë° ì£¼ í™”ì ì„ íƒ
        
        Args:
            audio_file: ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            audio_data: ì›ë³¸ ì˜¤ë””ì˜¤ ë°ì´í„°
            sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸
            
        Returns:
            ì£¼ í™”ìì˜ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” None
        """
        try:
            print("  ğŸ“¡ Pyannote diarization ì—”ì§„ ì‹œì‘...")
            
            # 16kHzë¡œ ë¦¬ìƒ˜í”Œë§ (pyannote ìš”êµ¬ì‚¬í•­)
            if sample_rate != 16000:
                resampled_audio = librosa.resample(audio_data, orig_sr=sample_rate, target_sr=16000)
                temp_16k_file = os.path.join(os.path.dirname(audio_file), 'temp_audio_16k.wav')
                
                # ì •ê·œí™”ëœ ì˜¤ë””ì˜¤ ì €ì¥
                if np.max(np.abs(resampled_audio)) > 1.0:
                    resampled_audio = resampled_audio / np.max(np.abs(resampled_audio)) * 0.95
                
                torchaudio.save(temp_16k_file, torch.from_numpy(resampled_audio).unsqueeze(0).float(), 16000)
                audio_file = temp_16k_file
                working_sample_rate = 16000
                working_audio = resampled_audio
            else:
                working_sample_rate = sample_rate
                working_audio = audio_data
            
            # Diarization íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (pyannote.audio 3.3.2) - ìºì‹± ì ìš©
            if self._diarization_pipeline is None:
                token = self.speaker_config.huggingface_token
                print("    ğŸ”„ Diarization ëª¨ë¸ ë¡œë”© ì¤‘...")
                
                # HuggingFace í† í° í™•ì¸
                if not token:
                    print("    âš ï¸  HuggingFace í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
                    print("    ğŸ’¡ í•´ê²°ë°©ë²•:")
                    print("       1. HuggingFace ê³„ì • ìƒì„±: https://huggingface.co/join")
                    print("       2. í† í° ìƒì„±: https://hf.co/settings/tokens")
                    print("       3. Gated ëª¨ë¸ ì ‘ê·¼ ìŠ¹ì¸: https://hf.co/pyannote/speaker-diarization-3.1")
                    print("    ğŸ”„ í† í° ì—†ì´ êµ¬ë²„ì „ ëª¨ë¸ë¡œ ì‹œë„...")

                try:
                    # ìµœì‹  ë²„ì „ API - pyannote.audio 3.3.2
                    if token:
                        print("    ğŸ“¡ ì‹œë„: speaker-diarization-3.1 (ìµœì‹ )")
                        self._diarization_pipeline = Pipeline.from_pretrained(
                            "pyannote/speaker-diarization-3.1", 
                            use_auth_token=token
                        )
                        if self._diarization_pipeline is not None:
                            print("    âœ… ìµœì‹  ëª¨ë¸ ë¡œë”© ì„±ê³µ!")
                        else:
                            print("    âŒ ìµœì‹  ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: None ë°˜í™˜")
                            raise ValueError("Pipeline returned None")
                    else:
                        raise ValueError("No token provided")
                        
                except Exception as e1:
                    print(f"    âš ï¸  ìµœì‹  ëª¨ë¸ ì‹¤íŒ¨: {str(e1)[:100]}...")
                    try:
                        # fallback to older model
                        print("    ğŸ“¡ ì‹œë„: speaker-diarization@2.1 (êµ¬ë²„ì „)")
                        self._diarization_pipeline = Pipeline.from_pretrained(
                            "pyannote/speaker-diarization@2.1", 
                            use_auth_token=token if token else None
                        )
                        if self._diarization_pipeline is not None:
                            print("    âœ… êµ¬ë²„ì „ ëª¨ë¸ ë¡œë”© ì„±ê³µ!")
                        else:
                            print("    âŒ êµ¬ë²„ì „ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: None ë°˜í™˜")
                            raise ValueError("Pipeline returned None")
                            
                    except Exception as e2:
                        print(f"    âš ï¸  êµ¬ë²„ì „ ëª¨ë¸ ì‹¤íŒ¨: {str(e2)[:100]}...")
                        try:
                            # fallback to token parameter name
                            if token:
                                print("    ğŸ“¡ ì‹œë„: í† í° íŒŒë¼ë¯¸í„° ë³€ê²½")
                                self._diarization_pipeline = Pipeline.from_pretrained(
                                    "pyannote/speaker-diarization-3.1", 
                                    token=token
                                )
                                if self._diarization_pipeline is not None:
                                    print("    âœ… í† í° ë³€ê²½ ì„±ê³µ!")
                                else:
                                    print("    âŒ í† í° ë³€ê²½ ì‹¤íŒ¨: None ë°˜í™˜")
                                    raise ValueError("Pipeline returned None")
                            else:
                                raise ValueError("No token for final attempt")
                                
                        except Exception as e3:
                            print(f"    âŒ ëª¨ë“  diarization ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨:")
                            print(f"       ìµœì‹  ëª¨ë¸: {str(e1)[:80]}...")
                            print(f"       êµ¬ë²„ì „ ëª¨ë¸: {str(e2)[:80]}...")
                            print(f"       í† í° ë³€ê²½: {str(e3)[:80]}...")
                            print("    ğŸ”„ ê³ ê¸‰ ëª¨ë¸ ì—†ì´ ê¸°ë³¸ ì—ë„ˆì§€ ê¸°ë°˜ ë¶„ë¦¬ë¡œ ì „í™˜ë©ë‹ˆë‹¤.")
                            self._diarization_pipeline = None
                            raise Exception("All diarization models failed to load")
                
                # pipelineì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œëœ ê²½ìš°ì—ë§Œ GPUë¡œ ì´ë™
                if self._diarization_pipeline is not None:
                    try:
                        # GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ GPUë¡œ ì´ë™
                        if torch.cuda.is_available():
                            self._diarization_pipeline.to(torch.device("cuda"))
                            print("    ğŸš€ GPU ëª¨ë“œ í™œì„±í™”")
                        else:
                            print("    ğŸ’» CPU ëª¨ë“œ ì‚¬ìš©")
                    except Exception as gpu_error:
                        print(f"    âš ï¸  GPU ì´ë™ ì‹¤íŒ¨, CPU ì‚¬ìš©: {gpu_error}")
                else:
                    print("    âŒ Pipelineì´ Noneì…ë‹ˆë‹¤ - ì´ˆê¸°í™” ì‹¤íŒ¨")
                    raise Exception("Diarization pipeline is None")
                
                print("    âœ… Diarization íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ!")
            
            pipeline = self._diarization_pipeline
            
            # Diarization ìˆ˜í–‰
            diarization = pipeline(audio_file)
            
            # í™”ìë³„ ì—ë„ˆì§€ ê³„ì‚°
            total_energy = defaultdict(float)
            speaker_names = []
            waveform_tensor = torch.from_numpy(working_audio).unsqueeze(0)
            
            for segment, _, speaker in diarization.itertracks(yield_label=True):
                speaker_names.append(speaker)
                
                start_sample = int(segment.start * working_sample_rate)
                end_sample = int(segment.end * working_sample_rate)
                segment_waveform = waveform_tensor[:, start_sample:end_sample]
                
                energy = torch.norm(segment_waveform)
                total_energy[speaker] += energy.item()
            
            speaker_names = list(set(speaker_names))
            
            # í™”ì ìˆ˜ í™•ì¸
            print(f"  ğŸ“Š ë°œê²¬ëœ í™”ì ìˆ˜: {len(speaker_names)}ëª…")
            for speaker in speaker_names:
                energy_ratio = total_energy[speaker] / max(total_energy.values()) if total_energy else 0
                print(f"    - {speaker}: ì—ë„ˆì§€ ë¹„ìœ¨ {energy_ratio:.2%}")
            
            if not total_energy or len(speaker_names) <= 1:
                print(f"  âš ï¸  í™”ì ìˆ˜ê°€ {len(speaker_names)}ëª…ì´ë¯€ë¡œ ë¶„ë¦¬í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ì£¼ í™”ì ì„ íƒ (ì—ë„ˆì§€ ê¸°ì¤€)
            main_speaker = max(total_energy, key=total_energy.get)
            energy_percentage = total_energy[main_speaker] / sum(total_energy.values()) * 100
            print(f"  ğŸ¯ ì£¼ í™”ì ì„ íƒ: {main_speaker} (ì—ë„ˆì§€ ì ìœ ìœ¨: {energy_percentage:.1f}%)")
            print(f"  ğŸ“ˆ ì´ í™”ì ìˆ˜: {len(speaker_names)}ëª…")
            
            # ì£¼ í™”ì ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ì§‘
            main_speaker_segments = []
            for segment, _, speaker in diarization.itertracks(yield_label=True):
                if speaker == main_speaker:
                    # ì›ë³¸ ìƒ˜í”Œë§ ë ˆì´íŠ¸ë¡œ ë³€í™˜
                    if sample_rate != 16000:
                        start_original = int(segment.start * sample_rate)
                        end_original = int(segment.end * sample_rate)
                    else:
                        start_original = int(segment.start * working_sample_rate)
                        end_original = int(segment.end * working_sample_rate)
                    
                    main_speaker_segments.append((start_original, end_original))
            
            # ì„ì‹œ 16kHz íŒŒì¼ ì •ë¦¬
            if sample_rate != 16000:
                temp_16k_file = os.path.join(os.path.dirname(audio_file), 'temp_audio_16k.wav')
                if os.path.exists(temp_16k_file):
                    os.remove(temp_16k_file)
            
            return main_speaker_segments
            
        except Exception as e:
            self.logger.error(f"Diarization ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _reduce_noise(self, audio_data: np.ndarray, sample_rate: int) -> np.ndarray:
        """
        ë…¸ì´ì¦ˆ ì œê±° ì²˜ë¦¬
        
        Args:
            audio_data: ìŒì„± ë°ì´í„°
            sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸
            
        Returns:
            ë…¸ì´ì¦ˆê°€ ì œê±°ëœ ìŒì„± ë°ì´í„°
        """
        try:
            self.logger.debug("ë…¸ì´ì¦ˆ ì œê±° ì‹œì‘")
            
            # ê¸°ë³¸ì ì¸ ìŠ¤í™íŠ¸ëŸ´ ê²Œì´íŒ… ê¸°ë°˜ ë…¸ì´ì¦ˆ ì œê±°
            # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë” ì •êµí•œ ë…¸ì´ì¦ˆ ì œê±° ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš© ê¶Œì¥
            
            # STFT ë³€í™˜
            stft = librosa.stft(audio_data, n_fft=2048, hop_length=512)
            magnitude = np.abs(stft)
            phase = np.angle(stft)
            
            # ë…¸ì´ì¦ˆ ì¶”ì • (ì²˜ìŒ 0.5ì´ˆë¥¼ ë…¸ì´ì¦ˆë¡œ ê°€ì •)
            noise_frames = int(0.5 * sample_rate / 512)  # hop_length=512
            if noise_frames > 0 and noise_frames < magnitude.shape[1]:
                noise_profile = np.mean(magnitude[:, :noise_frames], axis=1, keepdims=True)
            else:
                noise_profile = np.mean(magnitude, axis=1, keepdims=True) * 0.1
            
            # ìŠ¤í™íŠ¸ëŸ´ ê²Œì´íŒ…
            reduction_factor = self.noise_config.reduction_level
            mask = magnitude > (noise_profile * (1 + reduction_factor))
            
            # ë¶€ë“œëŸ¬ìš´ ë§ˆìŠ¤í‚¹ ì ìš©
            smooth_mask = np.maximum(mask.astype(float), 0.1)  # ìµœì†Œ 10% ìœ ì§€
            
            # ë§ˆìŠ¤í¬ ì ìš©
            cleaned_magnitude = magnitude * smooth_mask
            
            # ISTFTë¡œ ë³µì›
            cleaned_stft = cleaned_magnitude * np.exp(1j * phase)
            cleaned_audio = librosa.istft(cleaned_stft, hop_length=512)
            
            self.logger.debug("ë…¸ì´ì¦ˆ ì œê±° ì™„ë£Œ")
            return cleaned_audio
            
        except Exception as e:
            self.logger.warning(f"ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨, ì›ë³¸ ìŒì„± ì‚¬ìš©: {str(e)}")
            return audio_data
    
    def _extract_mel_features(self, audio_data: np.ndarray) -> np.ndarray:
        """
        Log-Mel spectrogram íŠ¹ì§• ì¶”ì¶œ
        
        Args:
            audio_data: 16kHz ìŒì„± ë°ì´í„°
            
        Returns:
            (80, 3000) í¬ê¸°ì˜ Log-Mel spectrogram
        """
        try:
            self.logger.debug("Mel íŠ¹ì§• ì¶”ì¶œ ì‹œì‘")
            
            # Whisper í‘œì¤€ ì„¤ì •ì— ë§ì¶˜ íŠ¹ì§• ì¶”ì¶œ
            # 30ì´ˆ ê¸¸ì´ë¡œ íŒ¨ë”© ë˜ëŠ” íŠ¸ë¦¼
            target_length = 16000 * 30  # 30ì´ˆ * 16kHz
            
            if len(audio_data) > target_length:
                # 30ì´ˆë³´ë‹¤ ê¸¸ë©´ íŠ¸ë¦¼
                audio_data = audio_data[:target_length]
            elif len(audio_data) < target_length:
                # 30ì´ˆë³´ë‹¤ ì§§ìœ¼ë©´ íŒ¨ë”©
                padding = target_length - len(audio_data)
                audio_data = np.pad(audio_data, (0, padding), mode='constant', constant_values=0)
            
            # Mel spectrogram ê³„ì‚°
            mel_spec = librosa.feature.melspectrogram(
                y=audio_data,
                sr=16000,
                n_fft=400,      # Whisper ì„¤ì •
                hop_length=160, # Whisper ì„¤ì •
                n_mels=80,      # Whisper ì„¤ì •
                fmin=0,
                fmax=8000
            )
            
            # Log ë³€í™˜
            log_mel = librosa.power_to_db(mel_spec, ref=np.max)
            
            # ì •ê·œí™” (-80dB ~ 0dBë¥¼ 0 ~ 1ë¡œ)
            log_mel = np.clip((log_mel + 80) / 80, 0, 1)
            
            # í¬ê¸° í™•ì¸ ë° ì¡°ì •
            if log_mel.shape[1] != 3000:
                if log_mel.shape[1] > 3000:
                    log_mel = log_mel[:, :3000]
                else:
                    padding = 3000 - log_mel.shape[1]
                    log_mel = np.pad(log_mel, ((0, 0), (0, padding)), mode='constant', constant_values=0)
            
            self.logger.debug(f"Mel íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ - í¬ê¸°: {log_mel.shape}")
            return log_mel
            
        except Exception as e:
            self.logger.error(f"Mel íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            raise AudioError(
                error_type=AudioErrorType.FEATURE_EXTRACTION_FAILED,
                message=f"íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}",
                details={'original_error': str(e)}
            )
    
    def _perform_sepformer_separation(self, audio_data: np.ndarray, sample_rate: int, main_speaker_segments: list) -> Optional[np.ndarray]:
        """
        SepFormerë¥¼ ì‚¬ìš©í•œ ìŒì„± ë¶„ë¦¬ ë° í™”ì ë§¤ì¹­
        """
        try:
            print("  ğŸ”Š SepFormer ìŒì„± ë¶„ë¦¬ ì—”ì§„ ì‹œì‘...")
            
            # 8kHzë¡œ ë¦¬ìƒ˜í”Œë§ (SepFormer ìš”êµ¬ì‚¬í•­)
            if sample_rate != 8000:
                resampled_audio = librosa.resample(audio_data, orig_sr=sample_rate, target_sr=8000)
            else:
                resampled_audio = audio_data
            
            # SepFormer ëª¨ë¸ ë¡œë“œ (speechbrain 1.0.3) - ìºì‹± ì ìš©
            if self._sepformer_model is None:
                device = "cuda" if torch.cuda.is_available() else "cpu"
                print(f"    ğŸ”„ SepFormer ëª¨ë¸ ë¡œë”© ì¤‘... (device: {device})")
                
                try:
                    # ìµœì‹  ë²„ì „ API with improved caching
                    print("    ğŸ“¡ speechbrain/sepformer-wsj02mix ë‹¤ìš´ë¡œë“œ ì¤‘...")
                    self._sepformer_model = separator.from_hparams(
                        source="speechbrain/sepformer-wsj02mix",
                        run_opts={"device": device},
                        savedir=os.path.join(self.models_cache_dir, "sepformer")
                    )
                    print("    âœ… ìµœì‹  APIë¡œ SepFormer ë¡œë”© ì„±ê³µ!")
                except Exception as e:
                    print(f"    âš ï¸  ìµœì‹  API ì‹¤íŒ¨: {str(e)[:50]}...")
                    # fallback to older API
                    print("    ğŸ”„ êµ¬ë²„ì „ APIë¡œ ì¬ì‹œë„...")
                    self._sepformer_model = separator.from_hparams(
                        source="speechbrain/sepformer-wsj02mix",
                        run_opts={"device": device}
                    )
                    print("    âœ… êµ¬ë²„ì „ APIë¡œ SepFormer ë¡œë”© ì„±ê³µ!")
                
                self._sepformer_model.eval()
                print("    âœ… SepFormer ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")
            
            model = self._sepformer_model
            
            # ìŒì„± ë¶„ë¦¬ ìˆ˜í–‰
            waveform_tensor = torch.from_numpy(resampled_audio).unsqueeze(0)
            if torch.cuda.is_available():
                waveform_tensor = waveform_tensor.to('cuda')
            
            print("    ğŸ”€ ìŒì„± ë¶„ë¦¬ ìˆ˜í–‰ ì¤‘...")
            estimated_sources = model(waveform_tensor)
            estimated_sources = estimated_sources.squeeze(0)
            num_speakers = estimated_sources.shape[1]
            print(f"    ğŸ“Š {num_speakers}ê°œ ìŒì„±ìœ¼ë¡œ ë¶„ë¦¬ ì™„ë£Œ!")
            
            # ë¶„ë¦¬ëœ ê° ìŒì„±ì„ ì •ê·œí™”
            separated_waveforms = []
            for i in range(num_speakers):
                waveform = estimated_sources[:, i].unsqueeze(0)
                waveform = rms_safe_normalize(waveform, max_target_rms=0.05)
                separated_waveforms.append(waveform)
            
            # í™”ì ì¸ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì£¼ í™”ìì™€ ë§¤ì¹­
            if self.speaker_config.speaker_recognition_enabled and SpeakerRecognition is not None:
                matched_audio = self._match_speaker_with_recognition(
                    separated_waveforms, main_speaker_segments, audio_data, sample_rate
                )
                if matched_audio is not None:
                    return matched_audio
            
            # í™”ì ì¸ì‹ì´ ì‹¤íŒ¨í•˜ë©´ ì²« ë²ˆì§¸ ë¶„ë¦¬ëœ ìŒì„± ë°˜í™˜
            if separated_waveforms:
                result_8k = separated_waveforms[0].cpu().numpy().squeeze()
                if sample_rate != 8000:
                    result = librosa.resample(result_8k, orig_sr=8000, target_sr=sample_rate)
                else:
                    result = result_8k
                return result
            
            return None
            
        except Exception as e:
            self.logger.error(f"SepFormer ë¶„ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _match_speaker_with_recognition(self, separated_waveforms: list, main_speaker_segments: list, 
                                       original_audio: np.ndarray, sample_rate: int) -> Optional[np.ndarray]:
        """
        ECAPA í™”ì ì¸ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ë¦¬ëœ ìŒì„±ê³¼ ì£¼ í™”ì ë§¤ì¹­
        """
        try:
            print("  ğŸ­ ECAPA í™”ì ì¸ì‹ì„ í†µí•œ ë§¤ì¹­ ì‹œì‘...")
            
            # í™”ì ì¸ì‹ ëª¨ë¸ ë¡œë“œ (speechbrain 1.0.3) - ìºì‹± ì ìš©
            if self._speaker_recognition_model is None:
                device = "cuda" if torch.cuda.is_available() else "cpu"
                print(f"    ğŸ”„ ECAPA ëª¨ë¸ ë¡œë”© ì¤‘... (device: {device})")
                
                try:
                    # ìµœì‹  ë²„ì „ API with improved caching
                    print("    ğŸ“¡ speechbrain/spkrec-ecapa-voxceleb ë‹¤ìš´ë¡œë“œ ì¤‘...")
                    self._speaker_recognition_model = SpeakerRecognition.from_hparams(
                        source="speechbrain/spkrec-ecapa-voxceleb",
                        run_opts={"device": device},
                        savedir=os.path.join(self.models_cache_dir, "ecapa")
                    )
                    print("    âœ… ìµœì‹  APIë¡œ ECAPA ë¡œë”© ì„±ê³µ!")
                except Exception as e:
                    print(f"    âš ï¸  ìµœì‹  API ì‹¤íŒ¨: {str(e)[:50]}...")
                    # fallback to older API
                    print("    ğŸ”„ êµ¬ë²„ì „ APIë¡œ ì¬ì‹œë„...")
                    self._speaker_recognition_model = SpeakerRecognition.from_hparams(
                        source="speechbrain/spkrec-ecapa-voxceleb",
                        run_opts={"device": device}
                    )
                    print("    âœ… êµ¬ë²„ì „ APIë¡œ ECAPA ë¡œë”© ì„±ê³µ!")
                
                print("    âœ… ECAPA ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")
            
            recognizer = self._speaker_recognition_model
            
            # ì£¼ í™”ì ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ì„ë² ë”© ì¶”ì¶œ (16kHz í•„ìš”)
            if sample_rate != 16000:
                resampled_original = librosa.resample(original_audio, orig_sr=sample_rate, target_sr=16000)
                working_sr = 16000
                main_segments_16k = [
                    (int(start * 16000 / sample_rate), int(end * 16000 / sample_rate))
                    for start, end in main_speaker_segments
                ]
            else:
                resampled_original = original_audio
                working_sr = 16000
                main_segments_16k = main_speaker_segments
            
            # ì£¼ í™”ì ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ ì—°ê²°
            main_speaker_audio_segments = []
            for start, end in main_segments_16k:
                segment = resampled_original[start:end]
                if len(segment) > 0:
                    main_speaker_audio_segments.append(segment)
            
            if not main_speaker_audio_segments:
                return None
            
            main_speaker_waveform = np.concatenate(main_speaker_audio_segments)
            main_speaker_tensor = torch.from_numpy(main_speaker_waveform).unsqueeze(0)
            
            if torch.cuda.is_available():
                main_speaker_tensor = main_speaker_tensor.to('cuda')
            
            # ì£¼ í™”ì ì„ë² ë”© ì¶”ì¶œ
            main_speaker_embedding = recognizer.encode_batch(main_speaker_tensor).squeeze()
            
            # ë¶„ë¦¬ëœ ê° ìŒì„±ì˜ ì„ë² ë”© ì¶”ì¶œ ë° ìœ ì‚¬ë„ ê³„ì‚°
            print("    ğŸ” ê° ë¶„ë¦¬ëœ ìŒì„±ê³¼ ì£¼ í™”ì ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
            similarities = []
            resampler_to_16k = torchaudio.transforms.Resample(orig_freq=8000, new_freq=16000)
            if torch.cuda.is_available():
                resampler_to_16k = resampler_to_16k.to('cuda')
            
            for i, waveform in enumerate(separated_waveforms):
                print(f"      ë¶„ë¦¬ìŒì„± {i+1} ì„ë² ë”© ì¶”ì¶œ ì¤‘...")
                resampled_waveform = resampler_to_16k(waveform)
                embedding = recognizer.encode_batch(resampled_waveform).squeeze()
                
                similarity = torch.nn.functional.cosine_similarity(
                    main_speaker_embedding, embedding, dim=0
                ).item()
                similarities.append(similarity)
                print(f"      ë¶„ë¦¬ìŒì„± {i+1} ìœ ì‚¬ë„: {similarity:.4f}")
            
            # ê°€ì¥ ìœ ì‚¬í•œ ìŒì„± ì„ íƒ
            best_match_idx = similarities.index(max(similarities))
            best_similarity = max(similarities)
            print(f"    ğŸ¯ ìµœê³  ìœ ì‚¬ë„: {best_similarity:.4f} (ë¶„ë¦¬ìŒì„± {best_match_idx+1} ì„ íƒ)")
            
            if best_similarity < 0.5:
                print(f"    âš ï¸  ìœ ì‚¬ë„ê°€ ë‚®ìŒ ({best_similarity:.4f} < 0.5) - ê²°ê³¼ í’ˆì§ˆ ì£¼ì˜")
            
            # ì„ íƒëœ ìŒì„±ì„ ì›ë³¸ ìƒ˜í”Œë§ ë ˆì´íŠ¸ë¡œ ë³€í™˜
            selected_waveform = separated_waveforms[best_match_idx].cpu().numpy().squeeze()
            if sample_rate != 8000:
                result = librosa.resample(selected_waveform, orig_sr=8000, target_sr=sample_rate)
            else:
                result = selected_waveform
            
            return result
            
        except Exception as e:
            self.logger.error(f"í™”ì ë§¤ì¹­ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _extract_main_speaker_from_segments(self, audio_data: np.ndarray, sample_rate: int, 
                                          main_speaker_segments: list) -> np.ndarray:
        """
        Diarization ê²°ê³¼ë¡œë¶€í„° ì£¼ í™”ì ì„¸ê·¸ë¨¼íŠ¸ë§Œ ì¶”ì¶œ
        """
        try:
            print("  âœ‚ï¸  ì£¼ í™”ì ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ ì¤‘...")
            segments = []
            total_duration = 0
            
            for i, (start, end) in enumerate(main_speaker_segments):
                segment = audio_data[start:end]
                if len(segment) > 0:
                    segments.append(segment)
                    duration = len(segment) / sample_rate
                    total_duration += duration
                    print(f"    ì„¸ê·¸ë¨¼íŠ¸ {i+1}: {duration:.2f}ì´ˆ")
            
            if segments:
                result = np.concatenate(segments)
                original_duration = len(audio_data) / sample_rate
                reduction_ratio = (original_duration - total_duration) / original_duration * 100
                print(f"  ğŸ“Š ì¶”ì¶œ ì™„ë£Œ - ì›ë³¸: {original_duration:.2f}ì´ˆ â†’ ê²°ê³¼: {total_duration:.2f}ì´ˆ")
                print(f"  ğŸ“‰ ìŒì„± ì••ì¶•ë¥ : {reduction_ratio:.1f}% ì œê±°")
                return result
            else:
                print("  âš ï¸  ìœ íš¨í•œ ì„¸ê·¸ë¨¼íŠ¸ ì—†ìŒ - ì›ë³¸ ë°˜í™˜")
                return audio_data
                
        except Exception as e:
            self.logger.error(f"ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return audio_data
    
    def _simple_speaker_separation(self, audio_data: np.ndarray, sample_rate: int) -> np.ndarray:
        """
        ê¸°ë³¸ ì—ë„ˆì§€ ê¸°ë°˜ í™”ì ì„ íƒ (ê¸°ì¡´ ë°©ì‹)
        """
        try:
            print("\nğŸ”‹ ê¸°ë³¸ ì—ë„ˆì§€ ê¸°ë°˜ í™”ì ë¶„ë¦¬ ì‹œì‘")
            print(f"ğŸ“Š ì…ë ¥: {len(audio_data)} ìƒ˜í”Œ, {sample_rate}Hz")
            
            # ìŒì„±ì„ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë‚˜ëˆ„ì–´ ì—ë„ˆì§€ ë¶„ì„
            segment_length = int(sample_rate * self.speaker_config.min_segment_length)
            segments = []
            
            for i in range(0, len(audio_data), segment_length):
                segment = audio_data[i:i + segment_length]
                if len(segment) > 0:
                    energy = np.sum(segment ** 2)
                    segments.append((i, segment, energy))
            
            if not segments:
                return audio_data
            
            # ì—ë„ˆì§€ê°€ ë†’ì€ ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ ì£¼ í™”ìë¡œ ê°„ì£¼
            segments.sort(key=lambda x: x[2], reverse=True)
            
            # ì„¤ì •ëœ ë¹„ìœ¨ë§Œí¼ ìƒìœ„ ì—ë„ˆì§€ ì„¸ê·¸ë¨¼íŠ¸ ì„ íƒ
            threshold_idx = max(1, int(len(segments) * (1 - self.speaker_config.energy_threshold_ratio)))
            selected_segments = segments[:threshold_idx]
            
            # ì‹œê°„ ìˆœì„œë¡œ ì¬ì •ë ¬
            selected_segments.sort(key=lambda x: x[0])
            
            # ì„ íƒëœ ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ ì—°ê²°
            result_audio = np.concatenate([seg[1] for seg in selected_segments])
            
            original_duration = len(audio_data) / sample_rate
            result_duration = len(result_audio) / sample_rate
            reduction_ratio = (original_duration - result_duration) / original_duration * 100
            
            print(f"ğŸ“Š ê¸°ë³¸ ë¶„ë¦¬ ì™„ë£Œ:")
            print(f"  - ì›ë³¸: {original_duration:.2f}ì´ˆ ({len(audio_data)} ìƒ˜í”Œ)")
            print(f"  - ê²°ê³¼: {result_duration:.2f}ì´ˆ ({len(result_audio)} ìƒ˜í”Œ)")
            print(f"  - ì••ì¶•ë¥ : {reduction_ratio:.1f}% ì œê±°")
            print(f"  - ì‚¬ìš©ëœ ì„¸ê·¸ë¨¼íŠ¸: {len(selected_segments)}/{len(segments)}ê°œ")
            
            return result_audio
            
        except Exception as e:
            self.logger.warning(f"ê¸°ë³¸ í™”ì ë¶„ë¦¬ ì‹¤íŒ¨, ì›ë³¸ ìŒì„± ì‚¬ìš©: {str(e)}")
            return audio_data
    
    def validate_audio_format(self, audio_data: np.ndarray, sample_rate: int) -> bool:
        """
        ìŒì„± í¬ë§· ê²€ì¦
        
        Args:
            audio_data: ìŒì„± ë°ì´í„°
            sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸
            
        Returns:
            ê²€ì¦ ê²°ê³¼
        """
        try:
            # ê¸°ë³¸ ê²€ì¦
            if audio_data is None or len(audio_data) == 0:
                return False
            
            if sample_rate <= 0:
                return False
            
            # ê¸¸ì´ ê²€ì¦ (ìµœëŒ€ 30ì´ˆ)
            duration = len(audio_data) / sample_rate
            if duration > 30.0:
                return False
            
            # ë™ì  ë²”ìœ„ ê²€ì¦
            if np.max(np.abs(audio_data)) == 0:
                return False
            
            return True
            
        except Exception:
            return False
    
    def get_audio_info(self, audio_input: AudioData) -> Dict[str, Any]:
        """
        ìŒì„± ì •ë³´ ë°˜í™˜
        
        Args:
            audio_input: ìŒì„± ë°ì´í„°
            
        Returns:
            ìŒì„± ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # ê¸°ë³¸ ì •ë³´
            info = {
                'duration': audio_input.duration,
                'sample_rate': audio_input.sample_rate,
                'channels': audio_input.channels,
                'samples': len(audio_input.data),
                'format_valid': self.validate_audio_format(audio_input.data, audio_input.sample_rate)
            }
            
            # ìŒì„± í’ˆì§ˆ ë¶„ì„
            if len(audio_input.data) > 0:
                info.update({
                    'rms_energy': float(np.sqrt(np.mean(audio_input.data ** 2))),
                    'peak_amplitude': float(np.max(np.abs(audio_input.data))),
                    'zero_crossing_rate': float(np.mean(librosa.feature.zero_crossing_rate(audio_input.data)[0])),
                    'spectral_centroid': float(np.mean(librosa.feature.spectral_centroid(
                        y=audio_input.data, sr=audio_input.sample_rate)[0]))
                })
            
            return info
            
        except Exception as e:
            self.logger.error(f"ìŒì„± ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return {
                'duration': audio_input.duration,
                'sample_rate': audio_input.sample_rate,
                'channels': audio_input.channels,
                'error': str(e)
            }
    
    def create_audio_data(self, file_path: str) -> AudioData:
        """
        íŒŒì¼ì—ì„œ AudioData ìƒì„±
        
        Args:
            file_path: ìŒì„± íŒŒì¼ ê²½ë¡œ
            
        Returns:
            AudioData ê°ì²´
            
        Raises:
            AudioError: íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ì‹œ
        """
        try:
            # librosaë¡œ ìŒì„± íŒŒì¼ ë¡œë“œ
            audio_data, sample_rate = librosa.load(file_path, sr=None, mono=True)
            
            duration = len(audio_data) / sample_rate
            channels = 1  # monoë¡œ ë¡œë“œí–ˆìœ¼ë¯€ë¡œ
            
            return AudioData(
                data=audio_data,
                sample_rate=sample_rate,
                channels=channels,
                duration=duration
            )
            
        except Exception as e:
            raise AudioError(
                error_type=AudioErrorType.FILE_LOAD_FAILED,
                message=f"ìŒì„± íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}",
                details={'file_path': file_path, 'original_error': str(e)}
            )


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_speaker_separation_system():
    """
    í™”ì ë¶„ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
    ë”ë¯¸ ë°ì´í„°ë¡œ ì‹œìŠ¤í…œ ì‘ë™ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    """
    print("\n" + "="*80)
    print("ğŸ§ª í™”ì ë¶„ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*80)
    
    try:
        # ë”ë¯¸ ì„¤ì • ìƒì„±
        from ..models.config_models import AudioConfig
        
        config = AudioConfig()
        processor = AudioProcessor(config)
        
        # ë”ë¯¸ ì˜¤ë””ì˜¤ ë°ì´í„° ìƒì„± (5ì´ˆ, 16kHz)
        import numpy as np
        duration = 5.0  # 5ì´ˆ
        sample_rate = 16000
        
        # 2ëª… í™”ìë¥¼ ì‹œë®¬ë ˆì´ì…˜í•œ ë”ë¯¸ ë°ì´í„°
        t = np.linspace(0, duration, int(duration * sample_rate), False)
        speaker1_freq = 440  # A note (ë” ê°•í•¨)
        speaker2_freq = 880  # A note 2ë°° (ë” ì•½í•¨)
        
        # ì²« ë²ˆì§¸ í™”ìê°€ ë” í° ì†Œë¦¬ë¡œ ë§í•˜ëŠ” ê²ƒì²˜ëŸ¼ ì‹œë®¬ë ˆì´ì…˜
        speaker1_signal = 0.7 * np.sin(speaker1_freq * 2.0 * np.pi * t)
        speaker2_signal = 0.3 * np.sin(speaker2_freq * 2.0 * np.pi * t)
        
        # ë…¸ì´ì¦ˆ ì¶”ê°€
        noise = 0.1 * np.random.randn(len(t))
        
        # í˜¼í•© ì‹ í˜¸ ìƒì„±
        mixed_audio = speaker1_signal + speaker2_signal + noise
        
        print(f"ğŸµ ë”ë¯¸ ì˜¤ë””ì˜¤ ìƒì„±: {duration}ì´ˆ, {sample_rate}Hz")
        print(f"ğŸ“Š í™”ì1 (440Hz): 70% ê°•ë„, í™”ì2 (880Hz): 30% ê°•ë„")
        
        # í™”ì ë¶„ë¦¬ í…ŒìŠ¤íŠ¸
        result = processor._separate_speakers(mixed_audio, sample_rate)
        
        result_duration = len(result) / sample_rate
        reduction = (duration - result_duration) / duration * 100
        
        print(f"\nğŸ¯ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"  âœ… ì‹œìŠ¤í…œ ì •ìƒ ì‘ë™")
        print(f"  ğŸ“Š ì›ë³¸: {duration:.2f}ì´ˆ â†’ ê²°ê³¼: {result_duration:.2f}ì´ˆ")
        print(f"  ğŸ“‰ ì••ì¶•ë¥ : {reduction:.1f}%")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        print("ğŸ”§ í•´ê²° ë°©ë²•:")
        print("  1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜: pip install -r requirements.txt")
        print("  2. HuggingFace í† í° ì„¤ì • í™•ì¸")
        print("  3. GPU/CUDA í™˜ê²½ í™•ì¸")
        return False
    
    finally:
        print("\n" + "="*80)
        print("ğŸ§ª í™”ì ë¶„ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print("="*80)


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ í˜¸ì¶œ
    test_speaker_separation_system()